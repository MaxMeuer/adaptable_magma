{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch distributed Default Port: 29500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "from activations.torch import Rational\n",
    "from clip.model import Bottleneck\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "# from .find_init\n",
    "# _weights import find_weights\n",
    "# from .warnings import RationalImportError\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "known_functions = {\n",
    "    \"relu\": lambda x: 0 if x < 0 else x,\n",
    "    \"leaky_relu\": lambda x: x/100 if x < 0 else x,\n",
    "    \"lrelu\": lambda x: x/100 if x < 0 else x,\n",
    "    \"normal\": lambda x: 1/np.sqrt(2*np.pi) * np.exp(-.5*x**2),\n",
    "}\n",
    "from magma.image_prefix import get_image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = get_image_encoder(\n",
    "            \"clip_RN50\",\n",
    "            convert_to_rational=True\n",
    "        ).to(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_rational_clip(enc):\n",
    "    for child in enc.children():\n",
    "        if isinstance(child, torch.nn.Sequential):\n",
    "            for seq_child in child.children():\n",
    "                if isinstance(seq_child, Bottleneck):\n",
    "                    for bot_params in seq_child.children():\n",
    "                        if isinstance(bot_params, (Rational, torch.nn.BatchNorm2d)):\n",
    "                            \n",
    "                            for param in bot_params.parameters():\n",
    "                                param.requires_grad = True\n",
    "\n",
    "                        else:\n",
    "                            for param in bot_params.parameters():\n",
    "                                param.requires_grad = False\n",
    "                else:\n",
    "                    for param in seq_child.parameters():\n",
    "                        param.requires_grad = False\n",
    "        else:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = freeze_rational_clip(enc).to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.float16\n",
      "bn1.weight torch.float32\n",
      "bn1.bias torch.float32\n",
      "conv2.weight torch.float16\n",
      "bn2.weight torch.float32\n",
      "bn2.bias torch.float32\n",
      "conv3.weight torch.float16\n",
      "bn3.weight torch.float32\n",
      "bn3.bias torch.float32\n",
      "layer1.0.conv1.weight torch.float16\n",
      "layer1.0.bn1.weight torch.float32\n",
      "layer1.0.bn1.bias torch.float32\n",
      "layer1.0.relu1.numerator torch.float32\n",
      "layer1.0.relu1.denominator torch.float32\n",
      "layer1.0.conv2.weight torch.float16\n",
      "layer1.0.bn2.weight torch.float32\n",
      "layer1.0.bn2.bias torch.float32\n",
      "layer1.0.relu2.numerator torch.float32\n",
      "layer1.0.relu2.denominator torch.float32\n",
      "layer1.0.conv3.weight torch.float16\n",
      "layer1.0.bn3.weight torch.float32\n",
      "layer1.0.bn3.bias torch.float32\n",
      "layer1.0.relu3.numerator torch.float32\n",
      "layer1.0.relu3.denominator torch.float32\n",
      "layer1.0.downsample.0.weight torch.float16\n",
      "layer1.0.downsample.1.weight torch.float32\n",
      "layer1.0.downsample.1.bias torch.float32\n",
      "layer1.1.conv1.weight torch.float16\n",
      "layer1.1.bn1.weight torch.float32\n",
      "layer1.1.bn1.bias torch.float32\n",
      "layer1.1.relu1.numerator torch.float32\n",
      "layer1.1.relu1.denominator torch.float32\n",
      "layer1.1.conv2.weight torch.float16\n",
      "layer1.1.bn2.weight torch.float32\n",
      "layer1.1.bn2.bias torch.float32\n",
      "layer1.1.relu2.numerator torch.float32\n",
      "layer1.1.relu2.denominator torch.float32\n",
      "layer1.1.conv3.weight torch.float16\n",
      "layer1.1.bn3.weight torch.float32\n",
      "layer1.1.bn3.bias torch.float32\n",
      "layer1.1.relu3.numerator torch.float32\n",
      "layer1.1.relu3.denominator torch.float32\n",
      "layer1.2.conv1.weight torch.float16\n",
      "layer1.2.bn1.weight torch.float32\n",
      "layer1.2.bn1.bias torch.float32\n",
      "layer1.2.relu1.numerator torch.float32\n",
      "layer1.2.relu1.denominator torch.float32\n",
      "layer1.2.conv2.weight torch.float16\n",
      "layer1.2.bn2.weight torch.float32\n",
      "layer1.2.bn2.bias torch.float32\n",
      "layer1.2.relu2.numerator torch.float32\n",
      "layer1.2.relu2.denominator torch.float32\n",
      "layer1.2.conv3.weight torch.float16\n",
      "layer1.2.bn3.weight torch.float32\n",
      "layer1.2.bn3.bias torch.float32\n",
      "layer1.2.relu3.numerator torch.float32\n",
      "layer1.2.relu3.denominator torch.float32\n",
      "layer2.0.conv1.weight torch.float16\n",
      "layer2.0.bn1.weight torch.float32\n",
      "layer2.0.bn1.bias torch.float32\n",
      "layer2.0.relu1.numerator torch.float32\n",
      "layer2.0.relu1.denominator torch.float32\n",
      "layer2.0.conv2.weight torch.float16\n",
      "layer2.0.bn2.weight torch.float32\n",
      "layer2.0.bn2.bias torch.float32\n",
      "layer2.0.relu2.numerator torch.float32\n",
      "layer2.0.relu2.denominator torch.float32\n",
      "layer2.0.conv3.weight torch.float16\n",
      "layer2.0.bn3.weight torch.float32\n",
      "layer2.0.bn3.bias torch.float32\n",
      "layer2.0.relu3.numerator torch.float32\n",
      "layer2.0.relu3.denominator torch.float32\n",
      "layer2.0.downsample.0.weight torch.float16\n",
      "layer2.0.downsample.1.weight torch.float32\n",
      "layer2.0.downsample.1.bias torch.float32\n",
      "layer2.1.conv1.weight torch.float16\n",
      "layer2.1.bn1.weight torch.float32\n",
      "layer2.1.bn1.bias torch.float32\n",
      "layer2.1.relu1.numerator torch.float32\n",
      "layer2.1.relu1.denominator torch.float32\n",
      "layer2.1.conv2.weight torch.float16\n",
      "layer2.1.bn2.weight torch.float32\n",
      "layer2.1.bn2.bias torch.float32\n",
      "layer2.1.relu2.numerator torch.float32\n",
      "layer2.1.relu2.denominator torch.float32\n",
      "layer2.1.conv3.weight torch.float16\n",
      "layer2.1.bn3.weight torch.float32\n",
      "layer2.1.bn3.bias torch.float32\n",
      "layer2.1.relu3.numerator torch.float32\n",
      "layer2.1.relu3.denominator torch.float32\n",
      "layer2.2.conv1.weight torch.float16\n",
      "layer2.2.bn1.weight torch.float32\n",
      "layer2.2.bn1.bias torch.float32\n",
      "layer2.2.relu1.numerator torch.float32\n",
      "layer2.2.relu1.denominator torch.float32\n",
      "layer2.2.conv2.weight torch.float16\n",
      "layer2.2.bn2.weight torch.float32\n",
      "layer2.2.bn2.bias torch.float32\n",
      "layer2.2.relu2.numerator torch.float32\n",
      "layer2.2.relu2.denominator torch.float32\n",
      "layer2.2.conv3.weight torch.float16\n",
      "layer2.2.bn3.weight torch.float32\n",
      "layer2.2.bn3.bias torch.float32\n",
      "layer2.2.relu3.numerator torch.float32\n",
      "layer2.2.relu3.denominator torch.float32\n",
      "layer2.3.conv1.weight torch.float16\n",
      "layer2.3.bn1.weight torch.float32\n",
      "layer2.3.bn1.bias torch.float32\n",
      "layer2.3.relu1.numerator torch.float32\n",
      "layer2.3.relu1.denominator torch.float32\n",
      "layer2.3.conv2.weight torch.float16\n",
      "layer2.3.bn2.weight torch.float32\n",
      "layer2.3.bn2.bias torch.float32\n",
      "layer2.3.relu2.numerator torch.float32\n",
      "layer2.3.relu2.denominator torch.float32\n",
      "layer2.3.conv3.weight torch.float16\n",
      "layer2.3.bn3.weight torch.float32\n",
      "layer2.3.bn3.bias torch.float32\n",
      "layer2.3.relu3.numerator torch.float32\n",
      "layer2.3.relu3.denominator torch.float32\n",
      "layer3.0.conv1.weight torch.float16\n",
      "layer3.0.bn1.weight torch.float32\n",
      "layer3.0.bn1.bias torch.float32\n",
      "layer3.0.relu1.numerator torch.float32\n",
      "layer3.0.relu1.denominator torch.float32\n",
      "layer3.0.conv2.weight torch.float16\n",
      "layer3.0.bn2.weight torch.float32\n",
      "layer3.0.bn2.bias torch.float32\n",
      "layer3.0.relu2.numerator torch.float32\n",
      "layer3.0.relu2.denominator torch.float32\n",
      "layer3.0.conv3.weight torch.float16\n",
      "layer3.0.bn3.weight torch.float32\n",
      "layer3.0.bn3.bias torch.float32\n",
      "layer3.0.relu3.numerator torch.float32\n",
      "layer3.0.relu3.denominator torch.float32\n",
      "layer3.0.downsample.0.weight torch.float16\n",
      "layer3.0.downsample.1.weight torch.float32\n",
      "layer3.0.downsample.1.bias torch.float32\n",
      "layer3.1.conv1.weight torch.float16\n",
      "layer3.1.bn1.weight torch.float32\n",
      "layer3.1.bn1.bias torch.float32\n",
      "layer3.1.relu1.numerator torch.float32\n",
      "layer3.1.relu1.denominator torch.float32\n",
      "layer3.1.conv2.weight torch.float16\n",
      "layer3.1.bn2.weight torch.float32\n",
      "layer3.1.bn2.bias torch.float32\n",
      "layer3.1.relu2.numerator torch.float32\n",
      "layer3.1.relu2.denominator torch.float32\n",
      "layer3.1.conv3.weight torch.float16\n",
      "layer3.1.bn3.weight torch.float32\n",
      "layer3.1.bn3.bias torch.float32\n",
      "layer3.1.relu3.numerator torch.float32\n",
      "layer3.1.relu3.denominator torch.float32\n",
      "layer3.2.conv1.weight torch.float16\n",
      "layer3.2.bn1.weight torch.float32\n",
      "layer3.2.bn1.bias torch.float32\n",
      "layer3.2.relu1.numerator torch.float32\n",
      "layer3.2.relu1.denominator torch.float32\n",
      "layer3.2.conv2.weight torch.float16\n",
      "layer3.2.bn2.weight torch.float32\n",
      "layer3.2.bn2.bias torch.float32\n",
      "layer3.2.relu2.numerator torch.float32\n",
      "layer3.2.relu2.denominator torch.float32\n",
      "layer3.2.conv3.weight torch.float16\n",
      "layer3.2.bn3.weight torch.float32\n",
      "layer3.2.bn3.bias torch.float32\n",
      "layer3.2.relu3.numerator torch.float32\n",
      "layer3.2.relu3.denominator torch.float32\n",
      "layer3.3.conv1.weight torch.float16\n",
      "layer3.3.bn1.weight torch.float32\n",
      "layer3.3.bn1.bias torch.float32\n",
      "layer3.3.relu1.numerator torch.float32\n",
      "layer3.3.relu1.denominator torch.float32\n",
      "layer3.3.conv2.weight torch.float16\n",
      "layer3.3.bn2.weight torch.float32\n",
      "layer3.3.bn2.bias torch.float32\n",
      "layer3.3.relu2.numerator torch.float32\n",
      "layer3.3.relu2.denominator torch.float32\n",
      "layer3.3.conv3.weight torch.float16\n",
      "layer3.3.bn3.weight torch.float32\n",
      "layer3.3.bn3.bias torch.float32\n",
      "layer3.3.relu3.numerator torch.float32\n",
      "layer3.3.relu3.denominator torch.float32\n",
      "layer3.4.conv1.weight torch.float16\n",
      "layer3.4.bn1.weight torch.float32\n",
      "layer3.4.bn1.bias torch.float32\n",
      "layer3.4.relu1.numerator torch.float32\n",
      "layer3.4.relu1.denominator torch.float32\n",
      "layer3.4.conv2.weight torch.float16\n",
      "layer3.4.bn2.weight torch.float32\n",
      "layer3.4.bn2.bias torch.float32\n",
      "layer3.4.relu2.numerator torch.float32\n",
      "layer3.4.relu2.denominator torch.float32\n",
      "layer3.4.conv3.weight torch.float16\n",
      "layer3.4.bn3.weight torch.float32\n",
      "layer3.4.bn3.bias torch.float32\n",
      "layer3.4.relu3.numerator torch.float32\n",
      "layer3.4.relu3.denominator torch.float32\n",
      "layer3.5.conv1.weight torch.float16\n",
      "layer3.5.bn1.weight torch.float32\n",
      "layer3.5.bn1.bias torch.float32\n",
      "layer3.5.relu1.numerator torch.float32\n",
      "layer3.5.relu1.denominator torch.float32\n",
      "layer3.5.conv2.weight torch.float16\n",
      "layer3.5.bn2.weight torch.float32\n",
      "layer3.5.bn2.bias torch.float32\n",
      "layer3.5.relu2.numerator torch.float32\n",
      "layer3.5.relu2.denominator torch.float32\n",
      "layer3.5.conv3.weight torch.float16\n",
      "layer3.5.bn3.weight torch.float32\n",
      "layer3.5.bn3.bias torch.float32\n",
      "layer3.5.relu3.numerator torch.float32\n",
      "layer3.5.relu3.denominator torch.float32\n",
      "layer4.0.conv1.weight torch.float16\n",
      "layer4.0.bn1.weight torch.float32\n",
      "layer4.0.bn1.bias torch.float32\n",
      "layer4.0.relu1.numerator torch.float32\n",
      "layer4.0.relu1.denominator torch.float32\n",
      "layer4.0.conv2.weight torch.float16\n",
      "layer4.0.bn2.weight torch.float32\n",
      "layer4.0.bn2.bias torch.float32\n",
      "layer4.0.relu2.numerator torch.float32\n",
      "layer4.0.relu2.denominator torch.float32\n",
      "layer4.0.conv3.weight torch.float16\n",
      "layer4.0.bn3.weight torch.float32\n",
      "layer4.0.bn3.bias torch.float32\n",
      "layer4.0.relu3.numerator torch.float32\n",
      "layer4.0.relu3.denominator torch.float32\n",
      "layer4.0.downsample.0.weight torch.float16\n",
      "layer4.0.downsample.1.weight torch.float32\n",
      "layer4.0.downsample.1.bias torch.float32\n",
      "layer4.1.conv1.weight torch.float16\n",
      "layer4.1.bn1.weight torch.float32\n",
      "layer4.1.bn1.bias torch.float32\n",
      "layer4.1.relu1.numerator torch.float32\n",
      "layer4.1.relu1.denominator torch.float32\n",
      "layer4.1.conv2.weight torch.float16\n",
      "layer4.1.bn2.weight torch.float32\n",
      "layer4.1.bn2.bias torch.float32\n",
      "layer4.1.relu2.numerator torch.float32\n",
      "layer4.1.relu2.denominator torch.float32\n",
      "layer4.1.conv3.weight torch.float16\n",
      "layer4.1.bn3.weight torch.float32\n",
      "layer4.1.bn3.bias torch.float32\n",
      "layer4.1.relu3.numerator torch.float32\n",
      "layer4.1.relu3.denominator torch.float32\n",
      "layer4.2.conv1.weight torch.float16\n",
      "layer4.2.bn1.weight torch.float32\n",
      "layer4.2.bn1.bias torch.float32\n",
      "layer4.2.relu1.numerator torch.float32\n",
      "layer4.2.relu1.denominator torch.float32\n",
      "layer4.2.conv2.weight torch.float16\n",
      "layer4.2.bn2.weight torch.float32\n",
      "layer4.2.bn2.bias torch.float32\n",
      "layer4.2.relu2.numerator torch.float32\n",
      "layer4.2.relu2.denominator torch.float32\n",
      "layer4.2.conv3.weight torch.float16\n",
      "layer4.2.bn3.weight torch.float32\n",
      "layer4.2.bn3.bias torch.float32\n",
      "layer4.2.relu3.numerator torch.float32\n",
      "layer4.2.relu3.denominator torch.float32\n"
     ]
    }
   ],
   "source": [
    "for i, (name, module) in enumerate(list(enc.named_parameters())):\n",
    "    print(name,module.dtype)\n",
    "    # if isinstance(module, torch.nn.BatchNorm2d): \n",
    "    #     prev_module = list(enc.named_modules())[i-1][1]\n",
    "    #     print(module.running_var.dtype)\n",
    "    #     enc.__setattr__(name, torch.nn.LayerNorm(module.weight.shape))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = enc.to(\"cuda:5\")\n",
    "x = torch.randn(1, 3, 224, 224).to('cuda:5', dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:5',\n",
       "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(rational_version, degrees, approx_func, k=None):\n",
    "    nd, dd = degrees\n",
    "    if approx_func == \"identity\":\n",
    "        return [0., 1.] + [0.] * (nd - 1), [0.] * dd\n",
    "    elif approx_func == \"ones\":\n",
    "        return [1.] * (nd + 1), [1.] * dd\n",
    "    rational_full_name = f\"Rational_version_{rational_version.upper()}{nd}/{dd}\"\n",
    "    if rational_version.lower() == \"rare\":\n",
    "        nd -= 2\n",
    "        rational_full_name += f\"_k_{k}\"\n",
    "    config_file = '../rationals_config.json'\n",
    "    config_file_dir = str(Path(os.path.abspath(__file__)).parent)\n",
    "    url = \"https://rational-activations.readthedocs.io/en/latest/tutorials/tutorials.1_find_weights_for_initialization.html\"\n",
    "    with open(os.path.join(config_file_dir, config_file)) as json_file:\n",
    "        rationals_dict = json.load(json_file)\n",
    "    if rational_full_name not in rationals_dict:\n",
    "        if approx_func.lower() in known_functions:\n",
    "            msg = f\"Found {approx_func} but haven't computed its rational approximation yet for degrees {degrees}.\\\n",
    "            \\nLet's do do it now:. \\n--> More info:\"\n",
    "            print(colored(msg, \"yellow\"))\n",
    "            print(colored(url, \"blue\"))\n",
    "            find_weights(known_functions[approx_func.lower()], function_name=approx_func.lower(), degrees=degrees, version=rational_version)\n",
    "            with open(os.path.join(config_file_dir, config_file)) as json_file:\n",
    "                rationals_dict = json.load(json_file)\n",
    "        else:\n",
    "            msg = f\"{rational_full_name} approximating \\\"{approx_func}\\\" not found in {config_file}.\\\n",
    "            \\nWe need to add it.\\nLet's do do it now. \\n--> More info:\"\n",
    "            print(colored(msg, \"yellow\"))\n",
    "            print(colored(url, \"blue\"))\n",
    "            find_weights(known_functions[approx_func.lower()], function_name=approx_func.lower(), degrees=degrees, version=rational_version)\n",
    "            with open(os.path.join(config_file_dir, config_file)) as json_file:\n",
    "                rationals_dict = json.load(json_file)\n",
    "    if approx_func not in rationals_dict[rational_full_name]:\n",
    "        if approx_func.lower() in known_functions:\n",
    "            msg = f\"Found {approx_func} but haven't computed its rational approximation yet for degrees {degrees}.\\\n",
    "            \\nLet's do do it now:. \\n--> More info:\"\n",
    "            print(colored(msg, \"yellow\"))\n",
    "            print(colored(url, \"blue\"))\n",
    "            find_weights(known_functions[approx_func.lower()], function_name=approx_func.lower(), degrees=degrees, version=rational_version)\n",
    "            with open(os.path.join(config_file_dir, config_file)) as json_file:\n",
    "                rationals_dict = json.load(json_file)\n",
    "        else:\n",
    "            msg = f\"{rational_full_name} approximating {approx_func} not found in {config_file}.\\\n",
    "            \\nWe need to add it.\\nLet's do do it now. \\n--> More info:\"\n",
    "            print(colored(msg, \"yellow\"))\n",
    "            print(colored(url, \"blue\"))\n",
    "    params = rationals_dict[rational_full_name][approx_func]\n",
    "    return params[\"init_w_numerator\"], params[\"init_w_denominator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rational_PYTORCH_A_F(x, weight_numerator, weight_denominator, device, pre, post):\n",
    "    # P(X) / Q(X) = a_0 + a_1 * X + ... + a_n * X^n /\n",
    "    #               1 + | b_1 * X | + | b_2 * X^2| + ... + | b_m * X ^m|\n",
    "\n",
    "    len_num, len_deno = len(weight_numerator), len(weight_denominator)\n",
    "\n",
    "    z = x.view(-1)\n",
    "\n",
    "    xps = torch.vander(z, N=max(len_num, len_deno), increasing=True)\n",
    "    numerator_mul = xps.mul(weight_numerator)\n",
    "    numerator = numerator_mul.sum(-1)\n",
    "    expanded_dw = torch.cat([pre, weight_denominator, post])\n",
    "    denominator = xps.mul(expanded_dw)\n",
    "    denominator_ab = denominator.abs()\n",
    "    denomi_sum = denominator_ab.sum(-1)\n",
    "\n",
    "    out = numerator.div(denomi_sum)\n",
    "    new_out = out.view(x.shape)\n",
    "    return new_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]])\n",
    "weight_numerator = torch.tensor([1.0, 2.0, 3.0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dd78b44acf79f242ecca224d05742a1b089d3d2b8416bfe26aa6b4466f63f70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
