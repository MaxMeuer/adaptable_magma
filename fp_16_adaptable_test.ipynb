{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4,6\n",
      "env: WORLD_SIZE=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=4,6\n",
    "%env WORLD_SIZE=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch distributed Default Port: 29500\n"
     ]
    }
   ],
   "source": [
    "import deepspeed \n",
    "from magma.magma import Magma\n",
    "from train import get_pretraining_datasets\n",
    "from magma.utils import configure_param_groups, load_model\n",
    "from magma.config import MultimodalConfig\n",
    "from magma.datasets import collate_fn\n",
    "from magma.config import MultimodalConfig\n",
    "from functools import partial\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-02 11:25:33,507] [INFO] [distributed.py:40:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...\n",
      "[2023-02-02 11:25:34,430] [INFO] [distributed.py:87:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=130.83.42.209, master_port=29500\n",
      "[2023-02-02 11:25:34,432] [INFO] [distributed.py:50:init_distributed] Initializing torch distributed with backend: nccl\n",
      "Loading GPTJ language model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading dataset paths from /home/ml-mmeuer/adaptable_magma/magma/datasets/coco_train_val: 5000it [00:00, 361478.21it/s]\n",
      "loading dataset paths from /home/ml-mmeuer/adaptable_magma/magma/datasets/coco_train_val: 5000it [00:00, 15532.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train dataset with 5000 samples\n",
      "Loaded eval dataset with 5000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deepspeed.init_distributed()\n",
    "model = Magma(\n",
    "            'fb20-dgx2-configs/dev.yml'\n",
    "        )\n",
    "tokenizer, config, transforms = model.tokenizer, model.config, model.transforms\n",
    "trainable_parameters = configure_param_groups(model, config)\n",
    "\n",
    "# load data:\n",
    "train_dataset, eval_dataset = get_pretraining_datasets(\n",
    "    config, tokenizer, transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ml-mmeuer/adaptable_magma/model_checkpoints/v1_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-02 11:27:07,452] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.5.10, git-hash=unknown, git-branch=unknown\n",
      "[2023-02-02 11:27:17,167] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed groups\n",
      "[2023-02-02 11:27:17,174] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1\n",
      "[2023-02-02 11:27:17,183] [INFO] [logging.py:69:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1\n",
      "[2023-02-02 11:27:17,184] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
      "[2023-02-02 11:27:17,185] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml-mmeuer/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-02 11:27:17,604] [INFO] [engine.py:275:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-02-02 11:27:17,605] [INFO] [engine.py:1091:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-02-02 11:27:17,612] [INFO] [engine.py:1097:_configure_optimizer] Using client Optimizer as basic optimizer\n",
      "[2023-02-02 11:27:17,633] [INFO] [engine.py:1113:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n",
      "[2023-02-02 11:27:17,635] [INFO] [utils.py:44:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2023-02-02 11:27:17,635] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
      "[2023-02-02 11:27:17,636] [INFO] [stage_1_and_2.py:121:__init__] Reduce bucket size 500000000\n",
      "[2023-02-02 11:27:17,636] [INFO] [stage_1_and_2.py:122:__init__] Allgather bucket size 500000000\n",
      "[2023-02-02 11:27:17,637] [INFO] [stage_1_and_2.py:123:__init__] CPU Offload: True\n",
      "[2023-02-02 11:27:17,637] [INFO] [stage_1_and_2.py:124:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ml-mmeuer/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ml-mmeuer/.cache/torch_extensions/py39_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load utils op: 0.7421677112579346 seconds\n",
      "Rank: 0 partition count [1] and sizes[(247619864, False)] \n",
      "[2023-02-02 11:27:21,790] [INFO] [utils.py:822:see_memory_usage] Before initializing optimizer states\n",
      "[2023-02-02 11:27:21,793] [INFO] [utils.py:823:see_memory_usage] MA 12.13 GB         Max_MA 12.13 GB         CA 12.6 GB         Max_CA 13 GB \n",
      "[2023-02-02 11:27:21,794] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 67.57 GB, percent = 6.7%\n",
      "[2023-02-02 11:27:23,232] [INFO] [utils.py:822:see_memory_usage] After initializing optimizer states\n",
      "[2023-02-02 11:27:23,234] [INFO] [utils.py:823:see_memory_usage] MA 12.13 GB         Max_MA 12.13 GB         CA 12.6 GB         Max_CA 13 GB \n",
      "[2023-02-02 11:27:23,236] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 70.37 GB, percent = 7.0%\n",
      "[2023-02-02 11:27:23,236] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized\n",
      "[2023-02-02 11:27:23,404] [INFO] [utils.py:822:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-02-02 11:27:23,406] [INFO] [utils.py:823:see_memory_usage] MA 12.13 GB         Max_MA 12.13 GB         CA 12.6 GB         Max_CA 13 GB \n",
      "[2023-02-02 11:27:23,407] [INFO] [utils.py:831:see_memory_usage] CPU Virtual Memory:  used = 70.36 GB, percent = 7.0%\n",
      "[2023-02-02 11:27:23,408] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2023-02-02 11:27:23,408] [INFO] [engine.py:795:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupDecayLR\n",
      "[2023-02-02 11:27:23,409] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fd9103901f0>\n",
      "[2023-02-02 11:27:23,409] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0008], mom=[(0.9, 0.95)]\n",
      "[2023-02-02 11:27:23,412] [INFO] [config.py:1058:print] DeepSpeedEngine configuration:\n",
      "[2023-02-02 11:27:23,413] [INFO] [config.py:1062:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-02-02 11:27:23,414] [INFO] [config.py:1062:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-02-02 11:27:23,414] [INFO] [config.py:1062:print]   amp_enabled .................. False\n",
      "[2023-02-02 11:27:23,415] [INFO] [config.py:1062:print]   amp_params ................... False\n",
      "[2023-02-02 11:27:23,416] [INFO] [config.py:1062:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-02-02 11:27:23,416] [INFO] [config.py:1062:print]   bfloat16_enabled ............. False\n",
      "[2023-02-02 11:27:23,417] [INFO] [config.py:1062:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-02-02 11:27:23,418] [INFO] [config.py:1062:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-02-02 11:27:23,419] [INFO] [config.py:1062:print]   communication_data_type ...... None\n",
      "[2023-02-02 11:27:23,419] [INFO] [config.py:1062:print]   curriculum_enabled ........... False\n",
      "[2023-02-02 11:27:23,420] [INFO] [config.py:1062:print]   curriculum_params ............ False\n",
      "[2023-02-02 11:27:23,421] [INFO] [config.py:1062:print]   dataloader_drop_last ......... False\n",
      "[2023-02-02 11:27:23,422] [INFO] [config.py:1062:print]   disable_allgather ............ False\n",
      "[2023-02-02 11:27:23,423] [INFO] [config.py:1062:print]   dump_state ................... False\n",
      "[2023-02-02 11:27:23,424] [INFO] [config.py:1062:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 250, 'delayed_shift': 2, 'min_scale': 1}\n",
      "[2023-02-02 11:27:23,426] [INFO] [config.py:1062:print]   eigenvalue_enabled ........... False\n",
      "[2023-02-02 11:27:23,426] [INFO] [config.py:1062:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-02-02 11:27:23,427] [INFO] [config.py:1062:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-02-02 11:27:23,427] [INFO] [config.py:1062:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-02-02 11:27:23,427] [INFO] [config.py:1062:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-02-02 11:27:23,428] [INFO] [config.py:1062:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-02-02 11:27:23,428] [INFO] [config.py:1062:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-02-02 11:27:23,429] [INFO] [config.py:1062:print]   eigenvalue_verbose ........... False\n",
      "[2023-02-02 11:27:23,429] [INFO] [config.py:1062:print]   elasticity_enabled ........... False\n",
      "[2023-02-02 11:27:23,429] [INFO] [config.py:1062:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-02-02 11:27:23,430] [INFO] [config.py:1062:print]   fp16_enabled ................. True\n",
      "[2023-02-02 11:27:23,430] [INFO] [config.py:1062:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-02-02 11:27:23,431] [INFO] [config.py:1062:print]   fp16_mixed_quantize .......... False\n",
      "[2023-02-02 11:27:23,431] [INFO] [config.py:1062:print]   global_rank .................. 0\n",
      "[2023-02-02 11:27:23,431] [INFO] [config.py:1062:print]   gradient_accumulation_steps .. 1\n",
      "[2023-02-02 11:27:23,432] [INFO] [config.py:1062:print]   gradient_clipping ............ 1.0\n",
      "[2023-02-02 11:27:23,432] [INFO] [config.py:1062:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-02-02 11:27:23,433] [INFO] [config.py:1062:print]   initial_dynamic_scale ........ 4294967296\n",
      "[2023-02-02 11:27:23,433] [INFO] [config.py:1062:print]   loss_scale ................... 0\n",
      "[2023-02-02 11:27:23,433] [INFO] [config.py:1062:print]   memory_breakdown ............. False\n",
      "[2023-02-02 11:27:23,434] [INFO] [config.py:1062:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-02-02 11:27:23,438] [INFO] [config.py:1062:print]   optimizer_name ............... None\n",
      "[2023-02-02 11:27:23,444] [INFO] [config.py:1062:print]   optimizer_params ............. None\n",
      "[2023-02-02 11:27:23,444] [INFO] [config.py:1062:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-02-02 11:27:23,445] [INFO] [config.py:1062:print]   pld_enabled .................. False\n",
      "[2023-02-02 11:27:23,446] [INFO] [config.py:1062:print]   pld_params ................... False\n",
      "[2023-02-02 11:27:23,446] [INFO] [config.py:1062:print]   prescale_gradients ........... False\n",
      "[2023-02-02 11:27:23,447] [INFO] [config.py:1062:print]   quantize_change_rate ......... 0.001\n",
      "[2023-02-02 11:27:23,448] [INFO] [config.py:1062:print]   quantize_groups .............. 1\n",
      "[2023-02-02 11:27:23,449] [INFO] [config.py:1062:print]   quantize_offset .............. 1000\n",
      "[2023-02-02 11:27:23,450] [INFO] [config.py:1062:print]   quantize_period .............. 1000\n",
      "[2023-02-02 11:27:23,450] [INFO] [config.py:1062:print]   quantize_rounding ............ 0\n",
      "[2023-02-02 11:27:23,451] [INFO] [config.py:1062:print]   quantize_start_bits .......... 16\n",
      "[2023-02-02 11:27:23,452] [INFO] [config.py:1062:print]   quantize_target_bits ......... 8\n",
      "[2023-02-02 11:27:23,453] [INFO] [config.py:1062:print]   quantize_training_enabled .... False\n",
      "[2023-02-02 11:27:23,454] [INFO] [config.py:1062:print]   quantize_type ................ 0\n",
      "[2023-02-02 11:27:23,455] [INFO] [config.py:1062:print]   quantize_verbose ............. False\n",
      "[2023-02-02 11:27:23,456] [INFO] [config.py:1062:print]   scheduler_name ............... WarmupDecayLR\n",
      "[2023-02-02 11:27:23,456] [INFO] [config.py:1062:print]   scheduler_params ............. {'total_num_steps': 300000, 'warmup_min_lr': 0.0, 'warmup_max_lr': 0.0008, 'warmup_num_steps': 100}\n",
      "[2023-02-02 11:27:23,457] [INFO] [config.py:1062:print]   sparse_attention ............. None\n",
      "[2023-02-02 11:27:23,457] [INFO] [config.py:1062:print]   sparse_gradients_enabled ..... False\n",
      "[2023-02-02 11:27:23,457] [INFO] [config.py:1062:print]   steps_per_print .............. 10\n",
      "[2023-02-02 11:27:23,458] [INFO] [config.py:1062:print]   tensorboard_enabled .......... False\n",
      "[2023-02-02 11:27:23,458] [INFO] [config.py:1062:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2023-02-02 11:27:23,458] [INFO] [config.py:1062:print]   tensorboard_output_path ...... \n",
      "[2023-02-02 11:27:23,459] [INFO] [config.py:1062:print]   train_batch_size ............. 2\n",
      "[2023-02-02 11:27:23,459] [INFO] [config.py:1062:print]   train_micro_batch_size_per_gpu  2\n",
      "[2023-02-02 11:27:23,459] [INFO] [config.py:1062:print]   use_quantizer_kernel ......... False\n",
      "[2023-02-02 11:27:23,460] [INFO] [config.py:1062:print]   wall_clock_breakdown ......... False\n",
      "[2023-02-02 11:27:23,460] [INFO] [config.py:1062:print]   world_size ................... 1\n",
      "[2023-02-02 11:27:23,460] [INFO] [config.py:1062:print]   zero_allow_untested_optimizer  False\n",
      "[2023-02-02 11:27:23,461] [INFO] [config.py:1062:print]   zero_config .................. {\n",
      "    \"stage\": 2, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+08, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": false, \n",
      "    \"load_from_fp32_weights\": false, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": {\n",
      "        \"device\": \"cpu\", \n",
      "        \"nvme_path\": null, \n",
      "        \"buffer_count\": 4, \n",
      "        \"pin_memory\": false, \n",
      "        \"pipeline_read\": false, \n",
      "        \"pipeline_write\": false, \n",
      "        \"fast_init\": false, \n",
      "        \"pipeline\": false\n",
      "    }, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_fp16_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2023-02-02 11:27:23,462] [INFO] [config.py:1062:print]   zero_enabled ................. True\n",
      "[2023-02-02 11:27:23,462] [INFO] [config.py:1062:print]   zero_optimization_stage ...... 2\n",
      "[2023-02-02 11:27:23,463] [INFO] [config.py:1064:print]   json = {\n",
      "    \"train_batch_size\": 2, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale_window\": 250\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupDecayLR\", \n",
      "        \"params\": {\n",
      "            \"total_num_steps\": 3.000000e+05, \n",
      "            \"warmup_min_lr\": 0.0, \n",
      "            \"warmup_max_lr\": 0.0008, \n",
      "            \"warmup_num_steps\": 100\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"load_from_fp32_weights\": false, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Time to load utils op: 0.000797271728515625 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ml-mmeuer/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-02 11:27:37,191] [INFO] [engine.py:2621:_get_all_zero_checkpoints] successfully read 2 ZeRO state_dicts for rank 0\n"
     ]
    },
    {
     "ename": "ZeRORuntimeException",
     "evalue": "The checkpoint being loaded used a DP world size of 2 but the current world size is 1. Automatic adjustment of ZeRO's optimizer state partitioning with a new world size is not currently supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeRORuntimeException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 22\u001b[0m\n\u001b[1;32m      4\u001b[0m opt \u001b[39m=\u001b[39m AdamW(\n\u001b[1;32m      5\u001b[0m         trainable_parameters,\n\u001b[1;32m      6\u001b[0m         config\u001b[39m.\u001b[39mlr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         eps\u001b[39m=\u001b[39m\u001b[39m1.0e-4\u001b[39m\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     12\u001b[0m model_engine, opt, train_loader, lr_scheduler \u001b[39m=\u001b[39m deepspeed\u001b[39m.\u001b[39minitialize(\n\u001b[1;32m     13\u001b[0m         \u001b[39m# args=config,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m         config_params\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mdeepspeed_config_params,\n\u001b[1;32m     20\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0m previous_global_step \u001b[39m=\u001b[39m load_model(\n\u001b[1;32m     23\u001b[0m             model_engine,\n\u001b[1;32m     24\u001b[0m             config\u001b[39m.\u001b[39;49mload,\n\u001b[1;32m     25\u001b[0m             load_optimizer_states\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mload_optimizer,\n\u001b[1;32m     26\u001b[0m             load_lr_scheduler_states\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mload_optimizer,\n\u001b[1;32m     27\u001b[0m         )\n",
      "File \u001b[0;32m~/adaptable_magma/magma/utils.py:118\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_engine, load_dir, load_optimizer_states, load_lr_scheduler_states)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mLoads a model from disk and returns the global step to resume from if loading was successful, otherwise returns 0\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     load_path, sd \u001b[39m=\u001b[39m model_engine\u001b[39m.\u001b[39;49mload_checkpoint(\n\u001b[1;32m    119\u001b[0m         load_dir,\n\u001b[1;32m    120\u001b[0m         load_module_strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    121\u001b[0m         load_optimizer_states\u001b[39m=\u001b[39;49mload_optimizer_states,\n\u001b[1;32m    122\u001b[0m         load_lr_scheduler_states\u001b[39m=\u001b[39;49mload_lr_scheduler_states,\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    125\u001b[0m     load_path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/deepspeed/runtime/engine.py:2399\u001b[0m, in \u001b[0;36mDeepSpeedEngine.load_checkpoint\u001b[0;34m(self, load_dir, tag, load_module_strict, load_optimizer_states, load_lr_scheduler_states, load_module_only)\u001b[0m\n\u001b[1;32m   2391\u001b[0m load_path, client_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_checkpoint(load_dir,\n\u001b[1;32m   2392\u001b[0m                                                  tag,\n\u001b[1;32m   2393\u001b[0m                                                  load_module_strict\u001b[39m=\u001b[39mload_module_strict,\n\u001b[1;32m   2394\u001b[0m                                                  load_optimizer_states\u001b[39m=\u001b[39mload_optimizer_states,\n\u001b[1;32m   2395\u001b[0m                                                  load_lr_scheduler_states\u001b[39m=\u001b[39mload_lr_scheduler_states,\n\u001b[1;32m   2396\u001b[0m                                                  load_module_only\u001b[39m=\u001b[39mload_module_only)\n\u001b[1;32m   2398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_optimization() \u001b[39mand\u001b[39;00m load_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2399\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_zero_checkpoint(\n\u001b[1;32m   2400\u001b[0m         load_dir,\n\u001b[1;32m   2401\u001b[0m         tag,\n\u001b[1;32m   2402\u001b[0m         load_optimizer_states\u001b[39m=\u001b[39;49mload_optimizer_states)\n\u001b[1;32m   2403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m success:\n\u001b[1;32m   2404\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39m_restore_from_fp16_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/deepspeed/runtime/engine.py:2540\u001b[0m, in \u001b[0;36mDeepSpeedEngine._load_zero_checkpoint\u001b[0;34m(self, load_dir, tag, load_optimizer_states)\u001b[0m\n\u001b[1;32m   2537\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m load_optimizer_states \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdp_world_size \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_checkpoint_dp_world_size:\n\u001b[0;32m-> 2540\u001b[0m     \u001b[39mraise\u001b[39;00m ZeRORuntimeException(\u001b[39m\"\u001b[39m\u001b[39mThe checkpoint being loaded used a DP \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m   2541\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mworld size of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_checkpoint_dp_world_size\u001b[39m}\u001b[39;00m\u001b[39m but the \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m   2542\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcurrent world size is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdp_world_size\u001b[39m}\u001b[39;00m\u001b[39m. Automatic adjustment \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m   2543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mof ZeRO\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms optimizer state partitioning with a new world size is not \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m   2544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcurrently supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mload_state_dict(\n\u001b[1;32m   2547\u001b[0m     state_dict_list\u001b[39m=\u001b[39mzero_sd_list,\n\u001b[1;32m   2548\u001b[0m     load_optimizer_states\u001b[39m=\u001b[39mload_optimizer_states,\n\u001b[1;32m   2549\u001b[0m     load_from_fp32_weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzero_load_from_fp32_weights(),\n\u001b[1;32m   2550\u001b[0m )\n\u001b[1;32m   2551\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2552\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(zero_sd_list)\u001b[39m}\u001b[39;00m\u001b[39m zero partition checkpoints for rank \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_rank\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2553\u001b[0m )\n",
      "\u001b[0;31mZeRORuntimeException\u001b[0m: The checkpoint being loaded used a DP world size of 2 but the current world size is 1. Automatic adjustment of ZeRO's optimizer state partitioning with a new world size is not currently supported."
     ]
    }
   ],
   "source": [
    "\n",
    "config = MultimodalConfig.from_yml(\n",
    "            'fb20-dgx2-configs/dev.yml'\n",
    "        )\n",
    "opt = AdamW(\n",
    "        trainable_parameters,\n",
    "        config.lr,\n",
    "        betas=(0.9, 0.95),\n",
    "        weight_decay=config.weight_decay,\n",
    "        eps=1.0e-4\n",
    "    )\n",
    "\n",
    "model_engine, opt, train_loader, lr_scheduler = deepspeed.initialize(\n",
    "        # args=config,\n",
    "        model=model,\n",
    "        optimizer=opt,\n",
    "        model_parameters=trainable_parameters,\n",
    "        training_data=train_dataset,\n",
    "        collate_fn=partial(collate_fn, seq_len=model.seq_len),\n",
    "        config_params=config.deepspeed_config_params,\n",
    "    )\n",
    "    \n",
    "previous_global_step = load_model(\n",
    "            model_engine,\n",
    "            config.load,\n",
    "            load_optimizer_states=config.load_optimizer,\n",
    "            load_lr_scheduler_states=config.load_optimizer,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch distributed Default Port: 29500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "import json\n",
    "import os\n",
    "import clip\n",
    "from PIL import Image\n",
    "import sys\n",
    "import inspect\n",
    "from activations.torch import Rational\n",
    "\n",
    "from activations.utils.get_weights import get_parameters\n",
    "from clip.model import Bottleneck\n",
    "from clip.clip import _transform\n",
    "from magma.transforms import clip_preprocess\n",
    "from magma.transforms import get_transforms\n",
    "\n",
    "from pathlib import Path\n",
    "# from .find_init\n",
    "# _weights import find_weights\n",
    "# from .warnings import RationalImportError\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "known_functions = {\n",
    "    \"relu\": lambda x: 0 if x < 0 else x,\n",
    "    \"leaky_relu\": lambda x: x/100 if x < 0 else x,\n",
    "    \"lrelu\": lambda x: x/100 if x < 0 else x,\n",
    "    \"normal\": lambda x: 1/np.sqrt(2*np.pi) * np.exp(-.5*x**2),\n",
    "}\n",
    "from magma.image_prefix import get_image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = get_image_encoder(\n",
    "            \"clip_resnet_large\",\n",
    "            convert_to_rational=True\n",
    "        ).to(\"cuda\")\n",
    "transform_magma = get_transforms(1, 'clip_resnet_large', input_resolution=224)\n",
    "transform_clip = _transform(224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_rational_clip(enc):\n",
    "    for child in enc.children():\n",
    "        if isinstance(child, torch.nn.Sequential):\n",
    "            for seq_child in child.children():\n",
    "                if isinstance(seq_child, Bottleneck):\n",
    "                    for bot_params in seq_child.children():\n",
    "                        if isinstance(bot_params, (Rational, torch.nn.BatchNorm2d)):\n",
    "                            \n",
    "                            for param in bot_params.parameters():\n",
    "                                param.requires_grad = True\n",
    "\n",
    "                        else:\n",
    "                            for param in bot_params.parameters():\n",
    "                                param.requires_grad = False\n",
    "                else:\n",
    "                    for param in seq_child.parameters():\n",
    "                        param.requires_grad = False\n",
    "        else:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = freeze_rational_clip(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./magma/datasets/coco_train_val/images/0/000000000139.jpg\").resize((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_magma = transform_magma(img).to(\"cuda\")\n",
    "x_clip = transform_clip(img)[None, ...].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(x_magma).isnan().any()\n",
    "enc(x_clip).isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModifiedResNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prefix \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m bottleneck \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m i , child \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mlist\u001b[39m(ModifiedResNet\u001b[39m.\u001b[39mchildren())): \n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[1;32m      5\u001b[0m         prefix\u001b[39m.\u001b[39mappend(child)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModifiedResNet' is not defined"
     ]
    }
   ],
   "source": [
    "prefix = []\n",
    "bottleneck = []\n",
    "for i , child in enumerate(list(ModifiedResNet.children())): \n",
    "    if i < 10:\n",
    "        prefix.append(child)\n",
    "    if i ==10:\n",
    "        bottleneck.append(list(list(child.children())[0].children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = torch.nn.Sequential(*prefix)\n",
    "bottleneck = torch.nn.Sequential(*bottleneck[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_numerator, w_denominator = get_parameters(\"A\", (5,4),\n",
    "                                            \"relu\")\n",
    "\n",
    "numerator = torch.nn.Parameter(torch.tensor(w_numerator, dtype=torch.float16),\n",
    "                                True).to('cuda')\n",
    "\n",
    "denominator = torch.nn.Parameter(torch.tensor(w_denominator, dtype=torch.float16),\n",
    "                                requires_grad=True).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_x = prefix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rational_PYTORCH_A_F(x, weight_numerator, weight_denominator):\n",
    "    # P(X) / Q(X) = a_0 + a_1 * X + ... + a_n * X^n /\n",
    "    #               1 + | b_1 * X | + | b_2 * X^2| + ... + | b_m * X ^m|\n",
    "    x_32 = x.\n",
    "    len_num, len_deno = len(weight_numerator), len(weight_denominator)\n",
    "    pre = torch.tensor([1.]).to(\n",
    "        device=x.device, dtype=torch.float32)  # .half()\n",
    "    post = torch.zeros(len(weight_numerator) -\n",
    "                       len(weight_denominator) - 1).to(device=x.device, dtype=torch.float32)\n",
    "\n",
    "    z = x_32.view(-1)\n",
    "    singles = z.view(z.shape[-1], 1)\n",
    "    pre_vander = singles.repeat(1, max(len_num, len_deno))\n",
    "    pows = torch.arange(0, max(len_num, len_deno),\n",
    "                        device=x.device, dtype=x.dtype)\n",
    "    torch.nan_to_num(pre_vander, nan=0, posinf=torch.finfo(x.dtype).max, neginf=torch.finfo(x.dtype).min)\n",
    "    vander = torch.pow(pre_vander, pows)\n",
    "    print(\"vander: \", vander.isinf().any() or vander.isnan().any())\n",
    "    numerator = torch.mul(vander, weight_numerator).sum(-1)\n",
    "\n",
    "    expanded_dw = torch.cat([pre, weight_denominator, post])\n",
    "\n",
    "    denominator = torch.mul(vander, expanded_dw).abs().sum(-1)\n",
    "\n",
    "    flat_out = torch.div(numerator, denominator)\n",
    "\n",
    "    out = flat_out.view(x_32.shape).to(x.dtype)\n",
    "    print(\"out: \", out.isinf().any() or out.isnan().any())\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vander:  tensor(False, device='cuda:0')\n",
      "out:  tensor(False, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rational_PYTORCH_A_F(normed_x,numerator, denominator).isinf().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "0\n",
      "tensor(-0.7773, device='cuda:0', dtype=torch.float16) tensor(0.5835, device='cuda:0', dtype=torch.float16)\n",
      "torch.float16\n",
      "----------\n",
      "1\n",
      "tensor(-8.4375, device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>) tensor(4.3711, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>)\n",
      "torch.float32\n",
      "----------\n",
      "2\n",
      "pows:  tensor(False, device='cuda:0')\n",
      "tensor(-1.8037, device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>) tensor(4.3008, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Rational' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m      8\u001b[0m      normed_x \u001b[39m=\u001b[39m out\n\u001b[0;32m----> 9\u001b[0m \u001b[39mprint\u001b[39m(child\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     10\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m out\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Rational' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "input  = (prefix(x))\n",
    "for i, (name,child) in enumerate(bottleneck.named_children()):\n",
    "     print(\"-\"*10)\n",
    "     print(name)\n",
    "     out = child(input)\n",
    "     print(out.min(), out.max())\n",
    "     if i ==1:\n",
    "          normed_x = out\n",
    "     print(child.weight.dtype)\n",
    "     input = out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 64, 1, 1], expected input[1, 256, 56, 56] to have 64 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bottleneck(prefix(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 64, 1, 1], expected input[1, 256, 56, 56] to have 64 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "bottleneck(prefix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_pre = torch.randn(prefix(x).shape, dtype=torch.float16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Rational,\n",
       " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Rational,\n",
       " Identity(),\n",
       " Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
       " BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Rational,\n",
       " Sequential(\n",
       "   (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "   (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "   (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       " )]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(list(mod_resnet.children())[10].children())[0].children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedResNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "      (downsample): Sequential(\n",
       "        (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "      (downsample): Sequential(\n",
       "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "      (downsample): Sequential(\n",
       "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "      (downsample): Sequential(\n",
       "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): Rational\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): Rational\n",
       "      (avgpool): Identity()\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu3): Rational\n",
       "    )\n",
       "  )\n",
       "  (attnpool): Lambda()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModifiedResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModifiedResNet = list(enc.named_modules())[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "ModifiedResNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "      (downsample): Sequential(\n",
      "        (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "      (downsample): Sequential(\n",
      "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "      (downsample): Sequential(\n",
      "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "      (downsample): Sequential(\n",
      "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "  )\n",
      "  (attnpool): Lambda()\n",
      ")\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<ReshapeAliasBackward0>)\n",
      "----------\n",
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 1, 49, 2048] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(module)\n\u001b[0;32m----> 5\u001b[0m out \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(out)\n\u001b[1;32m      7\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m out\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 1, 49, 2048] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "input = x\n",
    "for name, module in ModifiedResNet.named_modules(): \n",
    "    print(\"-\"*10)\n",
    "    print(module)\n",
    "    out = module(input)\n",
    "    print(out)\n",
    "    input = out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      " ModifiedResNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "      (downsample): Sequential(\n",
      "        (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "      (downsample): Sequential(\n",
      "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "      (downsample): Sequential(\n",
      "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "      (downsample): Sequential(\n",
      "        (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): Rational\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): Rational\n",
      "      (avgpool): Identity()\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): Rational\n",
      "    )\n",
      "  )\n",
      "  (attnpool): Lambda()\n",
      ")\n",
      "----------\n",
      "conv1 Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "----------\n",
      "bn1 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "----------\n",
      "relu1 ReLU(inplace=True)\n",
      "----------\n",
      "conv2 Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "----------\n",
      "bn2 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "----------\n",
      "relu2 ReLU(inplace=True)\n",
      "----------\n",
      "conv3 Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "----------\n",
      "bn3 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "----------\n",
      "relu3 ReLU(inplace=True)\n",
      "----------\n",
      "avgpool AvgPool2d(kernel_size=2, stride=2, padding=0)\n"
     ]
    }
   ],
   "source": [
    "for i,(name, module) in enumerate(list(enc.named_modules())): \n",
    "\n",
    "    initial_layers = []\n",
    "    firs_bottle_neck = []\n",
    "    if i < 11: \n",
    "        print(\"-\"*10)\n",
    "        print(name, module)\n",
    "        \n",
    "        initial_layers.append(module)\n",
    "    elif i > 10 and i < 49:\n",
    "        firs_bottle_neck.append(module)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firs_bottle_neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dd78b44acf79f242ecca224d05742a1b089d3d2b8416bfe26aa6b4466f63f70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
