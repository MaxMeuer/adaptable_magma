{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magma.magma import Magma \n",
    "import torch \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from activations.torch import Rational\n",
    "import numpy as np\n",
    "from magma.utils import get_tokenizer\n",
    "from magma.datasets import ImgCptDataset\n",
    "from magma.transforms import get_transforms\n",
    "from magma.config import MultimodalConfig\n",
    "config = MultimodalConfig.from_yml(\n",
    "            './../fb20-dgx2-configs/dev.yml'\n",
    "        )\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.color_palette('husl', 9)\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPTJ language model...\n",
      "From EleutherAI/gpt-j-6B\n",
      "loading magma checkpoint from: /home/ml-mmeuer/adaptable_magma/model_checkpoints/2_250.pt\n",
      "magma successfully loaded\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "transforms = get_transforms(0, 'clip', 384)\n",
    "\n",
    "new_250 = Magma.from_checkpoint(checkpoint_path='/home/ml-mmeuer/adaptable_magma/model_checkpoints/2_250.pt', config_path='./../fb20-dgx2-configs/dev.yml')    \n",
    "# new_500 = Magma.from_checkpoint(checkpoint_path='/home/ml-mmeuer/adaptable_magma/model_checkpoints/2_500.pt', config_path='./../fb20-dgx2-configs/dev.yml').cuda()       \n",
    "dataset = ImgCptDataset(config.eval_dataset_dir, new_250.tokenizer, new_250.transforms)\n",
    "# model_7250 = Magma(config='./../fb20-dgx2-configs/dev.yml').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm.transformer.wte.weight False\n",
      "lm.transformer.h.0.ln_1.weight False\n",
      "lm.transformer.h.0.ln_1.bias False\n",
      "lm.transformer.h.0.attn.k_proj.weight False\n",
      "lm.transformer.h.0.attn.v_proj.weight False\n",
      "lm.transformer.h.0.attn.q_proj.weight False\n",
      "lm.transformer.h.0.attn.out_proj.weight False\n",
      "lm.transformer.h.0.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.0.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.0.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.0.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.0.mlp.1.switch_logits True\n",
      "lm.transformer.h.0.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.0.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.0.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.0.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.0.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.0.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.1.ln_1.weight False\n",
      "lm.transformer.h.1.ln_1.bias False\n",
      "lm.transformer.h.1.attn.k_proj.weight False\n",
      "lm.transformer.h.1.attn.v_proj.weight False\n",
      "lm.transformer.h.1.attn.q_proj.weight False\n",
      "lm.transformer.h.1.attn.out_proj.weight False\n",
      "lm.transformer.h.1.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.1.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.1.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.1.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.1.mlp.1.switch_logits True\n",
      "lm.transformer.h.1.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.1.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.1.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.1.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.1.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.1.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.2.ln_1.weight False\n",
      "lm.transformer.h.2.ln_1.bias False\n",
      "lm.transformer.h.2.attn.k_proj.weight False\n",
      "lm.transformer.h.2.attn.v_proj.weight False\n",
      "lm.transformer.h.2.attn.q_proj.weight False\n",
      "lm.transformer.h.2.attn.out_proj.weight False\n",
      "lm.transformer.h.2.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.2.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.2.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.2.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.2.mlp.1.switch_logits True\n",
      "lm.transformer.h.2.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.2.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.2.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.2.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.2.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.2.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.3.ln_1.weight False\n",
      "lm.transformer.h.3.ln_1.bias False\n",
      "lm.transformer.h.3.attn.k_proj.weight False\n",
      "lm.transformer.h.3.attn.v_proj.weight False\n",
      "lm.transformer.h.3.attn.q_proj.weight False\n",
      "lm.transformer.h.3.attn.out_proj.weight False\n",
      "lm.transformer.h.3.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.3.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.3.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.3.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.3.mlp.1.switch_logits True\n",
      "lm.transformer.h.3.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.3.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.3.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.3.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.3.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.3.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.4.ln_1.weight False\n",
      "lm.transformer.h.4.ln_1.bias False\n",
      "lm.transformer.h.4.attn.k_proj.weight False\n",
      "lm.transformer.h.4.attn.v_proj.weight False\n",
      "lm.transformer.h.4.attn.q_proj.weight False\n",
      "lm.transformer.h.4.attn.out_proj.weight False\n",
      "lm.transformer.h.4.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.4.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.4.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.4.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.4.mlp.1.switch_logits True\n",
      "lm.transformer.h.4.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.4.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.4.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.4.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.4.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.4.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.5.ln_1.weight False\n",
      "lm.transformer.h.5.ln_1.bias False\n",
      "lm.transformer.h.5.attn.k_proj.weight False\n",
      "lm.transformer.h.5.attn.v_proj.weight False\n",
      "lm.transformer.h.5.attn.q_proj.weight False\n",
      "lm.transformer.h.5.attn.out_proj.weight False\n",
      "lm.transformer.h.5.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.5.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.5.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.5.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.5.mlp.1.switch_logits True\n",
      "lm.transformer.h.5.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.5.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.5.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.5.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.5.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.5.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.6.ln_1.weight False\n",
      "lm.transformer.h.6.ln_1.bias False\n",
      "lm.transformer.h.6.attn.k_proj.weight False\n",
      "lm.transformer.h.6.attn.v_proj.weight False\n",
      "lm.transformer.h.6.attn.q_proj.weight False\n",
      "lm.transformer.h.6.attn.out_proj.weight False\n",
      "lm.transformer.h.6.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.6.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.6.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.6.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.6.mlp.1.switch_logits True\n",
      "lm.transformer.h.6.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.6.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.6.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.6.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.6.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.6.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.7.ln_1.weight False\n",
      "lm.transformer.h.7.ln_1.bias False\n",
      "lm.transformer.h.7.attn.k_proj.weight False\n",
      "lm.transformer.h.7.attn.v_proj.weight False\n",
      "lm.transformer.h.7.attn.q_proj.weight False\n",
      "lm.transformer.h.7.attn.out_proj.weight False\n",
      "lm.transformer.h.7.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.7.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.7.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.7.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.7.mlp.1.switch_logits True\n",
      "lm.transformer.h.7.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.7.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.7.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.7.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.7.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.7.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.8.ln_1.weight False\n",
      "lm.transformer.h.8.ln_1.bias False\n",
      "lm.transformer.h.8.attn.k_proj.weight False\n",
      "lm.transformer.h.8.attn.v_proj.weight False\n",
      "lm.transformer.h.8.attn.q_proj.weight False\n",
      "lm.transformer.h.8.attn.out_proj.weight False\n",
      "lm.transformer.h.8.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.8.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.8.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.8.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.8.mlp.1.switch_logits True\n",
      "lm.transformer.h.8.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.8.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.8.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.8.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.8.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.8.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.9.ln_1.weight False\n",
      "lm.transformer.h.9.ln_1.bias False\n",
      "lm.transformer.h.9.attn.k_proj.weight False\n",
      "lm.transformer.h.9.attn.v_proj.weight False\n",
      "lm.transformer.h.9.attn.q_proj.weight False\n",
      "lm.transformer.h.9.attn.out_proj.weight False\n",
      "lm.transformer.h.9.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.9.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.9.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.9.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.9.mlp.1.switch_logits True\n",
      "lm.transformer.h.9.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.9.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.9.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.9.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.9.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.9.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.10.ln_1.weight False\n",
      "lm.transformer.h.10.ln_1.bias False\n",
      "lm.transformer.h.10.attn.k_proj.weight False\n",
      "lm.transformer.h.10.attn.v_proj.weight False\n",
      "lm.transformer.h.10.attn.q_proj.weight False\n",
      "lm.transformer.h.10.attn.out_proj.weight False\n",
      "lm.transformer.h.10.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.10.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.10.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.10.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.10.mlp.1.switch_logits True\n",
      "lm.transformer.h.10.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.10.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.10.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.10.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.10.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.10.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.11.ln_1.weight False\n",
      "lm.transformer.h.11.ln_1.bias False\n",
      "lm.transformer.h.11.attn.k_proj.weight False\n",
      "lm.transformer.h.11.attn.v_proj.weight False\n",
      "lm.transformer.h.11.attn.q_proj.weight False\n",
      "lm.transformer.h.11.attn.out_proj.weight False\n",
      "lm.transformer.h.11.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.11.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.11.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.11.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.11.mlp.1.switch_logits True\n",
      "lm.transformer.h.11.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.11.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.11.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.11.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.11.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.11.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.12.ln_1.weight False\n",
      "lm.transformer.h.12.ln_1.bias False\n",
      "lm.transformer.h.12.attn.k_proj.weight False\n",
      "lm.transformer.h.12.attn.v_proj.weight False\n",
      "lm.transformer.h.12.attn.q_proj.weight False\n",
      "lm.transformer.h.12.attn.out_proj.weight False\n",
      "lm.transformer.h.12.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.12.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.12.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.12.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.12.mlp.1.switch_logits True\n",
      "lm.transformer.h.12.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.12.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.12.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.12.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.12.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.12.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.13.ln_1.weight False\n",
      "lm.transformer.h.13.ln_1.bias False\n",
      "lm.transformer.h.13.attn.k_proj.weight False\n",
      "lm.transformer.h.13.attn.v_proj.weight False\n",
      "lm.transformer.h.13.attn.q_proj.weight False\n",
      "lm.transformer.h.13.attn.out_proj.weight False\n",
      "lm.transformer.h.13.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.13.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.13.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.13.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.13.mlp.1.switch_logits True\n",
      "lm.transformer.h.13.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.13.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.13.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.13.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.13.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.13.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.14.ln_1.weight False\n",
      "lm.transformer.h.14.ln_1.bias False\n",
      "lm.transformer.h.14.attn.k_proj.weight False\n",
      "lm.transformer.h.14.attn.v_proj.weight False\n",
      "lm.transformer.h.14.attn.q_proj.weight False\n",
      "lm.transformer.h.14.attn.out_proj.weight False\n",
      "lm.transformer.h.14.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.14.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.14.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.14.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.14.mlp.1.switch_logits True\n",
      "lm.transformer.h.14.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.14.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.14.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.14.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.14.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.14.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.15.ln_1.weight False\n",
      "lm.transformer.h.15.ln_1.bias False\n",
      "lm.transformer.h.15.attn.k_proj.weight False\n",
      "lm.transformer.h.15.attn.v_proj.weight False\n",
      "lm.transformer.h.15.attn.q_proj.weight False\n",
      "lm.transformer.h.15.attn.out_proj.weight False\n",
      "lm.transformer.h.15.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.15.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.15.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.15.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.15.mlp.1.switch_logits True\n",
      "lm.transformer.h.15.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.15.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.15.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.15.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.15.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.15.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.16.ln_1.weight False\n",
      "lm.transformer.h.16.ln_1.bias False\n",
      "lm.transformer.h.16.attn.k_proj.weight False\n",
      "lm.transformer.h.16.attn.v_proj.weight False\n",
      "lm.transformer.h.16.attn.q_proj.weight False\n",
      "lm.transformer.h.16.attn.out_proj.weight False\n",
      "lm.transformer.h.16.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.16.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.16.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.16.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.16.mlp.1.switch_logits True\n",
      "lm.transformer.h.16.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.16.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.16.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.16.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.16.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.16.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.17.ln_1.weight False\n",
      "lm.transformer.h.17.ln_1.bias False\n",
      "lm.transformer.h.17.attn.k_proj.weight False\n",
      "lm.transformer.h.17.attn.v_proj.weight False\n",
      "lm.transformer.h.17.attn.q_proj.weight False\n",
      "lm.transformer.h.17.attn.out_proj.weight False\n",
      "lm.transformer.h.17.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.17.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.17.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.17.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.17.mlp.1.switch_logits True\n",
      "lm.transformer.h.17.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.17.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.17.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.17.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.17.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.17.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.18.ln_1.weight False\n",
      "lm.transformer.h.18.ln_1.bias False\n",
      "lm.transformer.h.18.attn.k_proj.weight False\n",
      "lm.transformer.h.18.attn.v_proj.weight False\n",
      "lm.transformer.h.18.attn.q_proj.weight False\n",
      "lm.transformer.h.18.attn.out_proj.weight False\n",
      "lm.transformer.h.18.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.18.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.18.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.18.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.18.mlp.1.switch_logits True\n",
      "lm.transformer.h.18.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.18.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.18.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.18.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.18.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.18.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.19.ln_1.weight False\n",
      "lm.transformer.h.19.ln_1.bias False\n",
      "lm.transformer.h.19.attn.k_proj.weight False\n",
      "lm.transformer.h.19.attn.v_proj.weight False\n",
      "lm.transformer.h.19.attn.q_proj.weight False\n",
      "lm.transformer.h.19.attn.out_proj.weight False\n",
      "lm.transformer.h.19.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.19.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.19.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.19.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.19.mlp.1.switch_logits True\n",
      "lm.transformer.h.19.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.19.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.19.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.19.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.19.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.19.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.20.ln_1.weight False\n",
      "lm.transformer.h.20.ln_1.bias False\n",
      "lm.transformer.h.20.attn.k_proj.weight False\n",
      "lm.transformer.h.20.attn.v_proj.weight False\n",
      "lm.transformer.h.20.attn.q_proj.weight False\n",
      "lm.transformer.h.20.attn.out_proj.weight False\n",
      "lm.transformer.h.20.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.20.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.20.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.20.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.20.mlp.1.switch_logits True\n",
      "lm.transformer.h.20.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.20.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.20.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.20.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.20.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.20.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.21.ln_1.weight False\n",
      "lm.transformer.h.21.ln_1.bias False\n",
      "lm.transformer.h.21.attn.k_proj.weight False\n",
      "lm.transformer.h.21.attn.v_proj.weight False\n",
      "lm.transformer.h.21.attn.q_proj.weight False\n",
      "lm.transformer.h.21.attn.out_proj.weight False\n",
      "lm.transformer.h.21.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.21.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.21.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.21.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.21.mlp.1.switch_logits True\n",
      "lm.transformer.h.21.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.21.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.21.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.21.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.21.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.21.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.22.ln_1.weight False\n",
      "lm.transformer.h.22.ln_1.bias False\n",
      "lm.transformer.h.22.attn.k_proj.weight False\n",
      "lm.transformer.h.22.attn.v_proj.weight False\n",
      "lm.transformer.h.22.attn.q_proj.weight False\n",
      "lm.transformer.h.22.attn.out_proj.weight False\n",
      "lm.transformer.h.22.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.22.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.22.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.22.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.22.mlp.1.switch_logits True\n",
      "lm.transformer.h.22.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.22.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.22.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.22.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.22.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.22.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.23.ln_1.weight False\n",
      "lm.transformer.h.23.ln_1.bias False\n",
      "lm.transformer.h.23.attn.k_proj.weight False\n",
      "lm.transformer.h.23.attn.v_proj.weight False\n",
      "lm.transformer.h.23.attn.q_proj.weight False\n",
      "lm.transformer.h.23.attn.out_proj.weight False\n",
      "lm.transformer.h.23.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.23.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.23.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.23.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.23.mlp.1.switch_logits True\n",
      "lm.transformer.h.23.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.23.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.23.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.23.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.23.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.23.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.24.ln_1.weight False\n",
      "lm.transformer.h.24.ln_1.bias False\n",
      "lm.transformer.h.24.attn.k_proj.weight False\n",
      "lm.transformer.h.24.attn.v_proj.weight False\n",
      "lm.transformer.h.24.attn.q_proj.weight False\n",
      "lm.transformer.h.24.attn.out_proj.weight False\n",
      "lm.transformer.h.24.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.24.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.24.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.24.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.24.mlp.1.switch_logits True\n",
      "lm.transformer.h.24.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.24.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.24.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.24.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.24.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.24.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.25.ln_1.weight False\n",
      "lm.transformer.h.25.ln_1.bias False\n",
      "lm.transformer.h.25.attn.k_proj.weight False\n",
      "lm.transformer.h.25.attn.v_proj.weight False\n",
      "lm.transformer.h.25.attn.q_proj.weight False\n",
      "lm.transformer.h.25.attn.out_proj.weight False\n",
      "lm.transformer.h.25.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.25.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.25.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.25.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.25.mlp.1.switch_logits True\n",
      "lm.transformer.h.25.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.25.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.25.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.25.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.25.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.25.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.26.ln_1.weight False\n",
      "lm.transformer.h.26.ln_1.bias False\n",
      "lm.transformer.h.26.attn.k_proj.weight False\n",
      "lm.transformer.h.26.attn.v_proj.weight False\n",
      "lm.transformer.h.26.attn.q_proj.weight False\n",
      "lm.transformer.h.26.attn.out_proj.weight False\n",
      "lm.transformer.h.26.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.26.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.26.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.26.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.26.mlp.1.switch_logits True\n",
      "lm.transformer.h.26.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.26.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.26.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.26.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.26.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.26.mlp.1.adapter.2.bias True\n",
      "lm.transformer.h.27.ln_1.weight False\n",
      "lm.transformer.h.27.ln_1.bias False\n",
      "lm.transformer.h.27.attn.k_proj.weight False\n",
      "lm.transformer.h.27.attn.v_proj.weight False\n",
      "lm.transformer.h.27.attn.q_proj.weight False\n",
      "lm.transformer.h.27.attn.out_proj.weight False\n",
      "lm.transformer.h.27.mlp.0.fc_in.weight False\n",
      "lm.transformer.h.27.mlp.0.fc_in.bias False\n",
      "lm.transformer.h.27.mlp.0.fc_out.weight False\n",
      "lm.transformer.h.27.mlp.0.fc_out.bias False\n",
      "lm.transformer.h.27.mlp.1.switch_logits True\n",
      "lm.transformer.h.27.mlp.1.adapter.0.weight True\n",
      "lm.transformer.h.27.mlp.1.adapter.0.bias True\n",
      "lm.transformer.h.27.mlp.1.adapter.1.f.numerator True\n",
      "lm.transformer.h.27.mlp.1.adapter.1.f.denominator True\n",
      "lm.transformer.h.27.mlp.1.adapter.2.weight True\n",
      "lm.transformer.h.27.mlp.1.adapter.2.bias True\n",
      "lm.transformer.ln_f.weight False\n",
      "lm.transformer.ln_f.bias False\n",
      "lm.lm_head.weight False\n",
      "lm.lm_head.bias False\n",
      "image_prefix.enc.conv1.weight False\n",
      "image_prefix.enc.bn1.weight False\n",
      "image_prefix.enc.bn1.bias False\n",
      "image_prefix.enc.conv2.weight False\n",
      "image_prefix.enc.bn2.weight False\n",
      "image_prefix.enc.bn2.bias False\n",
      "image_prefix.enc.conv3.weight False\n",
      "image_prefix.enc.bn3.weight False\n",
      "image_prefix.enc.bn3.bias False\n",
      "image_prefix.enc.layer1.0.conv1.weight False\n",
      "image_prefix.enc.layer1.0.bn1.weight False\n",
      "image_prefix.enc.layer1.0.bn1.bias False\n",
      "image_prefix.enc.layer1.0.conv2.weight False\n",
      "image_prefix.enc.layer1.0.bn2.weight False\n",
      "image_prefix.enc.layer1.0.bn2.bias False\n",
      "image_prefix.enc.layer1.0.conv3.weight False\n",
      "image_prefix.enc.layer1.0.bn3.weight False\n",
      "image_prefix.enc.layer1.0.bn3.bias False\n",
      "image_prefix.enc.layer1.0.downsample.0.weight False\n",
      "image_prefix.enc.layer1.0.downsample.1.weight False\n",
      "image_prefix.enc.layer1.0.downsample.1.bias False\n",
      "image_prefix.enc.layer1.1.conv1.weight False\n",
      "image_prefix.enc.layer1.1.bn1.weight False\n",
      "image_prefix.enc.layer1.1.bn1.bias False\n",
      "image_prefix.enc.layer1.1.conv2.weight False\n",
      "image_prefix.enc.layer1.1.bn2.weight False\n",
      "image_prefix.enc.layer1.1.bn2.bias False\n",
      "image_prefix.enc.layer1.1.conv3.weight False\n",
      "image_prefix.enc.layer1.1.bn3.weight False\n",
      "image_prefix.enc.layer1.1.bn3.bias False\n",
      "image_prefix.enc.layer1.2.conv1.weight False\n",
      "image_prefix.enc.layer1.2.bn1.weight False\n",
      "image_prefix.enc.layer1.2.bn1.bias False\n",
      "image_prefix.enc.layer1.2.conv2.weight False\n",
      "image_prefix.enc.layer1.2.bn2.weight False\n",
      "image_prefix.enc.layer1.2.bn2.bias False\n",
      "image_prefix.enc.layer1.2.conv3.weight False\n",
      "image_prefix.enc.layer1.2.bn3.weight False\n",
      "image_prefix.enc.layer1.2.bn3.bias False\n",
      "image_prefix.enc.layer1.3.conv1.weight False\n",
      "image_prefix.enc.layer1.3.bn1.weight False\n",
      "image_prefix.enc.layer1.3.bn1.bias False\n",
      "image_prefix.enc.layer1.3.conv2.weight False\n",
      "image_prefix.enc.layer1.3.bn2.weight False\n",
      "image_prefix.enc.layer1.3.bn2.bias False\n",
      "image_prefix.enc.layer1.3.conv3.weight False\n",
      "image_prefix.enc.layer1.3.bn3.weight False\n",
      "image_prefix.enc.layer1.3.bn3.bias False\n",
      "image_prefix.enc.layer1.4.conv1.weight False\n",
      "image_prefix.enc.layer1.4.bn1.weight False\n",
      "image_prefix.enc.layer1.4.bn1.bias False\n",
      "image_prefix.enc.layer1.4.conv2.weight False\n",
      "image_prefix.enc.layer1.4.bn2.weight False\n",
      "image_prefix.enc.layer1.4.bn2.bias False\n",
      "image_prefix.enc.layer1.4.conv3.weight False\n",
      "image_prefix.enc.layer1.4.bn3.weight False\n",
      "image_prefix.enc.layer1.4.bn3.bias False\n",
      "image_prefix.enc.layer1.5.conv1.weight False\n",
      "image_prefix.enc.layer1.5.bn1.weight False\n",
      "image_prefix.enc.layer1.5.bn1.bias False\n",
      "image_prefix.enc.layer1.5.conv2.weight False\n",
      "image_prefix.enc.layer1.5.bn2.weight False\n",
      "image_prefix.enc.layer1.5.bn2.bias False\n",
      "image_prefix.enc.layer1.5.conv3.weight False\n",
      "image_prefix.enc.layer1.5.bn3.weight False\n",
      "image_prefix.enc.layer1.5.bn3.bias False\n",
      "image_prefix.enc.layer2.0.conv1.weight False\n",
      "image_prefix.enc.layer2.0.bn1.weight False\n",
      "image_prefix.enc.layer2.0.bn1.bias False\n",
      "image_prefix.enc.layer2.0.conv2.weight False\n",
      "image_prefix.enc.layer2.0.bn2.weight False\n",
      "image_prefix.enc.layer2.0.bn2.bias False\n",
      "image_prefix.enc.layer2.0.conv3.weight False\n",
      "image_prefix.enc.layer2.0.bn3.weight False\n",
      "image_prefix.enc.layer2.0.bn3.bias False\n",
      "image_prefix.enc.layer2.0.downsample.0.weight False\n",
      "image_prefix.enc.layer2.0.downsample.1.weight False\n",
      "image_prefix.enc.layer2.0.downsample.1.bias False\n",
      "image_prefix.enc.layer2.1.conv1.weight False\n",
      "image_prefix.enc.layer2.1.bn1.weight False\n",
      "image_prefix.enc.layer2.1.bn1.bias False\n",
      "image_prefix.enc.layer2.1.conv2.weight False\n",
      "image_prefix.enc.layer2.1.bn2.weight False\n",
      "image_prefix.enc.layer2.1.bn2.bias False\n",
      "image_prefix.enc.layer2.1.conv3.weight False\n",
      "image_prefix.enc.layer2.1.bn3.weight False\n",
      "image_prefix.enc.layer2.1.bn3.bias False\n",
      "image_prefix.enc.layer2.2.conv1.weight False\n",
      "image_prefix.enc.layer2.2.bn1.weight False\n",
      "image_prefix.enc.layer2.2.bn1.bias False\n",
      "image_prefix.enc.layer2.2.conv2.weight False\n",
      "image_prefix.enc.layer2.2.bn2.weight False\n",
      "image_prefix.enc.layer2.2.bn2.bias False\n",
      "image_prefix.enc.layer2.2.conv3.weight False\n",
      "image_prefix.enc.layer2.2.bn3.weight False\n",
      "image_prefix.enc.layer2.2.bn3.bias False\n",
      "image_prefix.enc.layer2.3.conv1.weight False\n",
      "image_prefix.enc.layer2.3.bn1.weight False\n",
      "image_prefix.enc.layer2.3.bn1.bias False\n",
      "image_prefix.enc.layer2.3.conv2.weight False\n",
      "image_prefix.enc.layer2.3.bn2.weight False\n",
      "image_prefix.enc.layer2.3.bn2.bias False\n",
      "image_prefix.enc.layer2.3.conv3.weight False\n",
      "image_prefix.enc.layer2.3.bn3.weight False\n",
      "image_prefix.enc.layer2.3.bn3.bias False\n",
      "image_prefix.enc.layer2.4.conv1.weight False\n",
      "image_prefix.enc.layer2.4.bn1.weight False\n",
      "image_prefix.enc.layer2.4.bn1.bias False\n",
      "image_prefix.enc.layer2.4.conv2.weight False\n",
      "image_prefix.enc.layer2.4.bn2.weight False\n",
      "image_prefix.enc.layer2.4.bn2.bias False\n",
      "image_prefix.enc.layer2.4.conv3.weight False\n",
      "image_prefix.enc.layer2.4.bn3.weight False\n",
      "image_prefix.enc.layer2.4.bn3.bias False\n",
      "image_prefix.enc.layer2.5.conv1.weight False\n",
      "image_prefix.enc.layer2.5.bn1.weight False\n",
      "image_prefix.enc.layer2.5.bn1.bias False\n",
      "image_prefix.enc.layer2.5.conv2.weight False\n",
      "image_prefix.enc.layer2.5.bn2.weight False\n",
      "image_prefix.enc.layer2.5.bn2.bias False\n",
      "image_prefix.enc.layer2.5.conv3.weight False\n",
      "image_prefix.enc.layer2.5.bn3.weight False\n",
      "image_prefix.enc.layer2.5.bn3.bias False\n",
      "image_prefix.enc.layer2.6.conv1.weight False\n",
      "image_prefix.enc.layer2.6.bn1.weight False\n",
      "image_prefix.enc.layer2.6.bn1.bias False\n",
      "image_prefix.enc.layer2.6.conv2.weight False\n",
      "image_prefix.enc.layer2.6.bn2.weight False\n",
      "image_prefix.enc.layer2.6.bn2.bias False\n",
      "image_prefix.enc.layer2.6.conv3.weight False\n",
      "image_prefix.enc.layer2.6.bn3.weight False\n",
      "image_prefix.enc.layer2.6.bn3.bias False\n",
      "image_prefix.enc.layer2.7.conv1.weight False\n",
      "image_prefix.enc.layer2.7.bn1.weight False\n",
      "image_prefix.enc.layer2.7.bn1.bias False\n",
      "image_prefix.enc.layer2.7.conv2.weight False\n",
      "image_prefix.enc.layer2.7.bn2.weight False\n",
      "image_prefix.enc.layer2.7.bn2.bias False\n",
      "image_prefix.enc.layer2.7.conv3.weight False\n",
      "image_prefix.enc.layer2.7.bn3.weight False\n",
      "image_prefix.enc.layer2.7.bn3.bias False\n",
      "image_prefix.enc.layer3.0.conv1.weight False\n",
      "image_prefix.enc.layer3.0.bn1.weight False\n",
      "image_prefix.enc.layer3.0.bn1.bias False\n",
      "image_prefix.enc.layer3.0.conv2.weight False\n",
      "image_prefix.enc.layer3.0.bn2.weight False\n",
      "image_prefix.enc.layer3.0.bn2.bias False\n",
      "image_prefix.enc.layer3.0.conv3.weight False\n",
      "image_prefix.enc.layer3.0.bn3.weight False\n",
      "image_prefix.enc.layer3.0.bn3.bias False\n",
      "image_prefix.enc.layer3.0.downsample.0.weight False\n",
      "image_prefix.enc.layer3.0.downsample.1.weight False\n",
      "image_prefix.enc.layer3.0.downsample.1.bias False\n",
      "image_prefix.enc.layer3.1.conv1.weight False\n",
      "image_prefix.enc.layer3.1.bn1.weight False\n",
      "image_prefix.enc.layer3.1.bn1.bias False\n",
      "image_prefix.enc.layer3.1.conv2.weight False\n",
      "image_prefix.enc.layer3.1.bn2.weight False\n",
      "image_prefix.enc.layer3.1.bn2.bias False\n",
      "image_prefix.enc.layer3.1.conv3.weight False\n",
      "image_prefix.enc.layer3.1.bn3.weight False\n",
      "image_prefix.enc.layer3.1.bn3.bias False\n",
      "image_prefix.enc.layer3.2.conv1.weight False\n",
      "image_prefix.enc.layer3.2.bn1.weight False\n",
      "image_prefix.enc.layer3.2.bn1.bias False\n",
      "image_prefix.enc.layer3.2.conv2.weight False\n",
      "image_prefix.enc.layer3.2.bn2.weight False\n",
      "image_prefix.enc.layer3.2.bn2.bias False\n",
      "image_prefix.enc.layer3.2.conv3.weight False\n",
      "image_prefix.enc.layer3.2.bn3.weight False\n",
      "image_prefix.enc.layer3.2.bn3.bias False\n",
      "image_prefix.enc.layer3.3.conv1.weight False\n",
      "image_prefix.enc.layer3.3.bn1.weight False\n",
      "image_prefix.enc.layer3.3.bn1.bias False\n",
      "image_prefix.enc.layer3.3.conv2.weight False\n",
      "image_prefix.enc.layer3.3.bn2.weight False\n",
      "image_prefix.enc.layer3.3.bn2.bias False\n",
      "image_prefix.enc.layer3.3.conv3.weight False\n",
      "image_prefix.enc.layer3.3.bn3.weight False\n",
      "image_prefix.enc.layer3.3.bn3.bias False\n",
      "image_prefix.enc.layer3.4.conv1.weight False\n",
      "image_prefix.enc.layer3.4.bn1.weight False\n",
      "image_prefix.enc.layer3.4.bn1.bias False\n",
      "image_prefix.enc.layer3.4.conv2.weight False\n",
      "image_prefix.enc.layer3.4.bn2.weight False\n",
      "image_prefix.enc.layer3.4.bn2.bias False\n",
      "image_prefix.enc.layer3.4.conv3.weight False\n",
      "image_prefix.enc.layer3.4.bn3.weight False\n",
      "image_prefix.enc.layer3.4.bn3.bias False\n",
      "image_prefix.enc.layer3.5.conv1.weight False\n",
      "image_prefix.enc.layer3.5.bn1.weight False\n",
      "image_prefix.enc.layer3.5.bn1.bias False\n",
      "image_prefix.enc.layer3.5.conv2.weight False\n",
      "image_prefix.enc.layer3.5.bn2.weight False\n",
      "image_prefix.enc.layer3.5.bn2.bias False\n",
      "image_prefix.enc.layer3.5.conv3.weight False\n",
      "image_prefix.enc.layer3.5.bn3.weight False\n",
      "image_prefix.enc.layer3.5.bn3.bias False\n",
      "image_prefix.enc.layer3.6.conv1.weight False\n",
      "image_prefix.enc.layer3.6.bn1.weight False\n",
      "image_prefix.enc.layer3.6.bn1.bias False\n",
      "image_prefix.enc.layer3.6.conv2.weight False\n",
      "image_prefix.enc.layer3.6.bn2.weight False\n",
      "image_prefix.enc.layer3.6.bn2.bias False\n",
      "image_prefix.enc.layer3.6.conv3.weight False\n",
      "image_prefix.enc.layer3.6.bn3.weight False\n",
      "image_prefix.enc.layer3.6.bn3.bias False\n",
      "image_prefix.enc.layer3.7.conv1.weight False\n",
      "image_prefix.enc.layer3.7.bn1.weight False\n",
      "image_prefix.enc.layer3.7.bn1.bias False\n",
      "image_prefix.enc.layer3.7.conv2.weight False\n",
      "image_prefix.enc.layer3.7.bn2.weight False\n",
      "image_prefix.enc.layer3.7.bn2.bias False\n",
      "image_prefix.enc.layer3.7.conv3.weight False\n",
      "image_prefix.enc.layer3.7.bn3.weight False\n",
      "image_prefix.enc.layer3.7.bn3.bias False\n",
      "image_prefix.enc.layer3.8.conv1.weight False\n",
      "image_prefix.enc.layer3.8.bn1.weight False\n",
      "image_prefix.enc.layer3.8.bn1.bias False\n",
      "image_prefix.enc.layer3.8.conv2.weight False\n",
      "image_prefix.enc.layer3.8.bn2.weight False\n",
      "image_prefix.enc.layer3.8.bn2.bias False\n",
      "image_prefix.enc.layer3.8.conv3.weight False\n",
      "image_prefix.enc.layer3.8.bn3.weight False\n",
      "image_prefix.enc.layer3.8.bn3.bias False\n",
      "image_prefix.enc.layer3.9.conv1.weight False\n",
      "image_prefix.enc.layer3.9.bn1.weight False\n",
      "image_prefix.enc.layer3.9.bn1.bias False\n",
      "image_prefix.enc.layer3.9.conv2.weight False\n",
      "image_prefix.enc.layer3.9.bn2.weight False\n",
      "image_prefix.enc.layer3.9.bn2.bias False\n",
      "image_prefix.enc.layer3.9.conv3.weight False\n",
      "image_prefix.enc.layer3.9.bn3.weight False\n",
      "image_prefix.enc.layer3.9.bn3.bias False\n",
      "image_prefix.enc.layer3.10.conv1.weight False\n",
      "image_prefix.enc.layer3.10.bn1.weight False\n",
      "image_prefix.enc.layer3.10.bn1.bias False\n",
      "image_prefix.enc.layer3.10.conv2.weight False\n",
      "image_prefix.enc.layer3.10.bn2.weight False\n",
      "image_prefix.enc.layer3.10.bn2.bias False\n",
      "image_prefix.enc.layer3.10.conv3.weight False\n",
      "image_prefix.enc.layer3.10.bn3.weight False\n",
      "image_prefix.enc.layer3.10.bn3.bias False\n",
      "image_prefix.enc.layer3.11.conv1.weight False\n",
      "image_prefix.enc.layer3.11.bn1.weight False\n",
      "image_prefix.enc.layer3.11.bn1.bias False\n",
      "image_prefix.enc.layer3.11.conv2.weight False\n",
      "image_prefix.enc.layer3.11.bn2.weight False\n",
      "image_prefix.enc.layer3.11.bn2.bias False\n",
      "image_prefix.enc.layer3.11.conv3.weight False\n",
      "image_prefix.enc.layer3.11.bn3.weight False\n",
      "image_prefix.enc.layer3.11.bn3.bias False\n",
      "image_prefix.enc.layer3.12.conv1.weight False\n",
      "image_prefix.enc.layer3.12.bn1.weight False\n",
      "image_prefix.enc.layer3.12.bn1.bias False\n",
      "image_prefix.enc.layer3.12.conv2.weight False\n",
      "image_prefix.enc.layer3.12.bn2.weight False\n",
      "image_prefix.enc.layer3.12.bn2.bias False\n",
      "image_prefix.enc.layer3.12.conv3.weight False\n",
      "image_prefix.enc.layer3.12.bn3.weight False\n",
      "image_prefix.enc.layer3.12.bn3.bias False\n",
      "image_prefix.enc.layer3.13.conv1.weight False\n",
      "image_prefix.enc.layer3.13.bn1.weight False\n",
      "image_prefix.enc.layer3.13.bn1.bias False\n",
      "image_prefix.enc.layer3.13.conv2.weight False\n",
      "image_prefix.enc.layer3.13.bn2.weight False\n",
      "image_prefix.enc.layer3.13.bn2.bias False\n",
      "image_prefix.enc.layer3.13.conv3.weight False\n",
      "image_prefix.enc.layer3.13.bn3.weight False\n",
      "image_prefix.enc.layer3.13.bn3.bias False\n",
      "image_prefix.enc.layer3.14.conv1.weight False\n",
      "image_prefix.enc.layer3.14.bn1.weight False\n",
      "image_prefix.enc.layer3.14.bn1.bias False\n",
      "image_prefix.enc.layer3.14.conv2.weight False\n",
      "image_prefix.enc.layer3.14.bn2.weight False\n",
      "image_prefix.enc.layer3.14.bn2.bias False\n",
      "image_prefix.enc.layer3.14.conv3.weight False\n",
      "image_prefix.enc.layer3.14.bn3.weight False\n",
      "image_prefix.enc.layer3.14.bn3.bias False\n",
      "image_prefix.enc.layer3.15.conv1.weight False\n",
      "image_prefix.enc.layer3.15.bn1.weight False\n",
      "image_prefix.enc.layer3.15.bn1.bias False\n",
      "image_prefix.enc.layer3.15.conv2.weight False\n",
      "image_prefix.enc.layer3.15.bn2.weight False\n",
      "image_prefix.enc.layer3.15.bn2.bias False\n",
      "image_prefix.enc.layer3.15.conv3.weight False\n",
      "image_prefix.enc.layer3.15.bn3.weight False\n",
      "image_prefix.enc.layer3.15.bn3.bias False\n",
      "image_prefix.enc.layer3.16.conv1.weight False\n",
      "image_prefix.enc.layer3.16.bn1.weight False\n",
      "image_prefix.enc.layer3.16.bn1.bias False\n",
      "image_prefix.enc.layer3.16.conv2.weight False\n",
      "image_prefix.enc.layer3.16.bn2.weight False\n",
      "image_prefix.enc.layer3.16.bn2.bias False\n",
      "image_prefix.enc.layer3.16.conv3.weight False\n",
      "image_prefix.enc.layer3.16.bn3.weight False\n",
      "image_prefix.enc.layer3.16.bn3.bias False\n",
      "image_prefix.enc.layer3.17.conv1.weight False\n",
      "image_prefix.enc.layer3.17.bn1.weight False\n",
      "image_prefix.enc.layer3.17.bn1.bias False\n",
      "image_prefix.enc.layer3.17.conv2.weight False\n",
      "image_prefix.enc.layer3.17.bn2.weight False\n",
      "image_prefix.enc.layer3.17.bn2.bias False\n",
      "image_prefix.enc.layer3.17.conv3.weight False\n",
      "image_prefix.enc.layer3.17.bn3.weight False\n",
      "image_prefix.enc.layer3.17.bn3.bias False\n",
      "image_prefix.enc.layer4.0.conv1.weight False\n",
      "image_prefix.enc.layer4.0.bn1.weight False\n",
      "image_prefix.enc.layer4.0.bn1.bias False\n",
      "image_prefix.enc.layer4.0.conv2.weight False\n",
      "image_prefix.enc.layer4.0.bn2.weight False\n",
      "image_prefix.enc.layer4.0.bn2.bias False\n",
      "image_prefix.enc.layer4.0.conv3.weight False\n",
      "image_prefix.enc.layer4.0.bn3.weight False\n",
      "image_prefix.enc.layer4.0.bn3.bias False\n",
      "image_prefix.enc.layer4.0.downsample.0.weight False\n",
      "image_prefix.enc.layer4.0.downsample.1.weight False\n",
      "image_prefix.enc.layer4.0.downsample.1.bias False\n",
      "image_prefix.enc.layer4.1.conv1.weight False\n",
      "image_prefix.enc.layer4.1.bn1.weight False\n",
      "image_prefix.enc.layer4.1.bn1.bias False\n",
      "image_prefix.enc.layer4.1.conv2.weight False\n",
      "image_prefix.enc.layer4.1.bn2.weight False\n",
      "image_prefix.enc.layer4.1.bn2.bias False\n",
      "image_prefix.enc.layer4.1.conv3.weight False\n",
      "image_prefix.enc.layer4.1.bn3.weight False\n",
      "image_prefix.enc.layer4.1.bn3.bias False\n",
      "image_prefix.enc.layer4.2.conv1.weight False\n",
      "image_prefix.enc.layer4.2.bn1.weight False\n",
      "image_prefix.enc.layer4.2.bn1.bias False\n",
      "image_prefix.enc.layer4.2.conv2.weight False\n",
      "image_prefix.enc.layer4.2.bn2.weight False\n",
      "image_prefix.enc.layer4.2.bn2.bias False\n",
      "image_prefix.enc.layer4.2.conv3.weight False\n",
      "image_prefix.enc.layer4.2.bn3.weight False\n",
      "image_prefix.enc.layer4.2.bn3.bias False\n",
      "image_prefix.enc.layer4.3.conv1.weight False\n",
      "image_prefix.enc.layer4.3.bn1.weight False\n",
      "image_prefix.enc.layer4.3.bn1.bias False\n",
      "image_prefix.enc.layer4.3.conv2.weight False\n",
      "image_prefix.enc.layer4.3.bn2.weight False\n",
      "image_prefix.enc.layer4.3.bn2.bias False\n",
      "image_prefix.enc.layer4.3.conv3.weight False\n",
      "image_prefix.enc.layer4.3.bn3.weight False\n",
      "image_prefix.enc.layer4.3.bn3.bias False\n",
      "image_prefix.enc.layer4.4.conv1.weight False\n",
      "image_prefix.enc.layer4.4.bn1.weight False\n",
      "image_prefix.enc.layer4.4.bn1.bias False\n",
      "image_prefix.enc.layer4.4.conv2.weight False\n",
      "image_prefix.enc.layer4.4.bn2.weight False\n",
      "image_prefix.enc.layer4.4.bn2.bias False\n",
      "image_prefix.enc.layer4.4.conv3.weight False\n",
      "image_prefix.enc.layer4.4.bn3.weight False\n",
      "image_prefix.enc.layer4.4.bn3.bias False\n",
      "image_prefix.enc.layer4.5.conv1.weight False\n",
      "image_prefix.enc.layer4.5.bn1.weight False\n",
      "image_prefix.enc.layer4.5.bn1.bias False\n",
      "image_prefix.enc.layer4.5.conv2.weight False\n",
      "image_prefix.enc.layer4.5.bn2.weight False\n",
      "image_prefix.enc.layer4.5.bn2.bias False\n",
      "image_prefix.enc.layer4.5.conv3.weight False\n",
      "image_prefix.enc.layer4.5.bn3.weight False\n",
      "image_prefix.enc.layer4.5.bn3.bias False\n",
      "image_prefix.enc.layer4.6.conv1.weight False\n",
      "image_prefix.enc.layer4.6.bn1.weight False\n",
      "image_prefix.enc.layer4.6.bn1.bias False\n",
      "image_prefix.enc.layer4.6.conv2.weight False\n",
      "image_prefix.enc.layer4.6.bn2.weight False\n",
      "image_prefix.enc.layer4.6.bn2.bias False\n",
      "image_prefix.enc.layer4.6.conv3.weight False\n",
      "image_prefix.enc.layer4.6.bn3.weight False\n",
      "image_prefix.enc.layer4.6.bn3.bias False\n",
      "image_prefix.enc.layer4.7.conv1.weight False\n",
      "image_prefix.enc.layer4.7.bn1.weight False\n",
      "image_prefix.enc.layer4.7.bn1.bias False\n",
      "image_prefix.enc.layer4.7.conv2.weight False\n",
      "image_prefix.enc.layer4.7.bn2.weight False\n",
      "image_prefix.enc.layer4.7.bn2.bias False\n",
      "image_prefix.enc.layer4.7.conv3.weight False\n",
      "image_prefix.enc.layer4.7.bn3.weight False\n",
      "image_prefix.enc.layer4.7.bn3.bias False\n",
      "image_prefix.proj.weight True\n",
      "image_prefix.proj.bias True\n",
      "image_prefix.ln.weight True\n",
      "image_prefix.ln.bias True\n"
     ]
    }
   ],
   "source": [
    "for n,m in new_250.named_parameters(): \n",
    "    print(n, m.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dist_forwards(model, dataset, num_samples=10):\n",
    "    def grad_hook(module, input, output):\n",
    "        module.input.append( input[0].detach().cpu().tolist())\n",
    "        \n",
    "    for i,(n,m) in enumerate(model.named_modules()):\n",
    "        if isinstance(m, Rational):\n",
    "            setattr(m,'input', [])\n",
    "            m.register_forward_hook(grad_hook)\n",
    "            \n",
    "    for i in range(num_samples):\n",
    "        images , captions = dataset[i]\n",
    "        images, captions = images, captions\n",
    "        model.forward(images, captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dynamic_fig(model,cols=2 ,title='Dynamic Softmax', switch_temp=1.0,type=Rational):\n",
    "    i=0\n",
    "    for n,p in model.named_modules():\n",
    "        if isinstance(p, type):\n",
    "            i += cols\n",
    "        rows = i // cols + 1\n",
    "        rest = i % cols\n",
    "        if rest != 0:\n",
    "            rows += 1  \n",
    "            \n",
    "    Position = range(1,i + 1)\n",
    "\n",
    "    fig = plt.figure(1, figsize=(20,80))\n",
    "\n",
    "    fig.suptitle(title, fontsize=20, )\n",
    "    return fig, Position, rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_dist(model, cols=2, title='Distribution over input of rational activations'):\n",
    "    fig, Position, rows, cols = return_dynamic_fig(model,cols=2, title=\"Title\")\n",
    "    j = 0\n",
    "    for n,m in model.named_modules():\n",
    "        if isinstance(m, Rational):\n",
    "            linsp = torch.linspace(-3,3,100)\n",
    "            y = m.forward(linsp)\n",
    "\n",
    "            ax1 = fig.add_subplot(rows,cols,Position[j])\n",
    "            ax2 = ax1.twinx()\n",
    "            input_data = m.input.detach().numpy()\n",
    "            input_filter_range = np.where((input_data > -3) & (input_data < 3))\n",
    "            sns.distplot(m.input, ax=ax1, color='red')\n",
    "            sns.lineplot(x=linsp.numpy(), y=y.numpy(), ax=ax2, color='blue')\n",
    "            j+= 1\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"slow_conv2d_cpu\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_dist_forwards(new_250, dataset, num_samples\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m plot_input_dist(new_250, \u001b[39m2\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDistribution over input of rational activations\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [18], line 13\u001b[0m, in \u001b[0;36minput_dist_forwards\u001b[0;34m(model, dataset, num_samples)\u001b[0m\n\u001b[1;32m     11\u001b[0m images , captions \u001b[39m=\u001b[39m dataset[i]\n\u001b[1;32m     12\u001b[0m images, captions \u001b[39m=\u001b[39m images, captions\n\u001b[0;32m---> 13\u001b[0m model\u001b[39m.\u001b[39;49mforward(images, captions)\n",
      "File \u001b[0;32m~/adaptable_magma/magma/magma.py:353\u001b[0m, in \u001b[0;36mMagma.forward\u001b[0;34m(self, images, captions, output_hidden_states, input_embeddings)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    349\u001b[0m     captions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len\n\u001b[1;32m    350\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min training, captions should be padded to sequence length (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len\u001b[39m}\u001b[39;00m\u001b[39m), but are length \u001b[39m\u001b[39m{\u001b[39;00mcaptions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m \u001b[39mif\u001b[39;00m input_embeddings \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     input_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_prefix(images)\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m images\u001b[39m.\u001b[39mdim \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m:\n\u001b[1;32m    356\u001b[0m     images \u001b[39m=\u001b[39m images[:, :, \u001b[39mNone\u001b[39;00m, :, :, :]  \u001b[39m# Add Times Dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/adaptable_magma/magma/image_prefix.py:98\u001b[0m, in \u001b[0;36mImagePrefix.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     94\u001b[0m     \u001b[39mself\u001b[39m, x: TensorType[\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mh\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     95\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorType[\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mseq\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mout_dim\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     96\u001b[0m     \u001b[39m# pass through image encoder\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc(x)\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mcross_attention_config:\n\u001b[1;32m    100\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/clip/model.py:147\u001b[0m, in \u001b[0;36mModifiedResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m    146\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 147\u001b[0m x \u001b[39m=\u001b[39m stem(x)\n\u001b[1;32m    148\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m    149\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/clip/model.py:140\u001b[0m, in \u001b[0;36mModifiedResNet.forward.<locals>.stem\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstem\u001b[39m(x):\n\u001b[0;32m--> 140\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m    141\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m    142\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"slow_conv2d_cpu\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "input_dist_forwards(new_250, dataset, num_samples=10)\n",
    "plot_input_dist(new_250, 2, title='Distribution over input of rational activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "------------------------\n",
      "1\n",
      "------------------------\n",
      "2\n",
      "------------------------\n",
      "3\n",
      "------------------------\n",
      "4\n",
      "------------------------\n",
      "5\n",
      "------------------------\n",
      "6\n",
      "------------------------\n",
      "7\n",
      "------------------------\n",
      "8\n",
      "------------------------\n",
      "9\n",
      "------------------------\n",
      "10\n",
      "------------------------\n",
      "11\n",
      "------------------------\n",
      "12\n",
      "------------------------\n",
      "13\n",
      "------------------------\n",
      "14\n",
      "------------------------\n",
      "15\n",
      "------------------------\n",
      "16\n",
      "------------------------\n",
      "17\n",
      "------------------------\n",
      "18\n",
      "------------------------\n",
      "19\n",
      "------------------------\n",
      "20\n",
      "------------------------\n",
      "21\n",
      "------------------------\n",
      "22\n",
      "------------------------\n",
      "23\n",
      "------------------------\n",
      "24\n",
      "------------------------\n",
      "25\n",
      "------------------------\n",
      "26\n",
      "------------------------\n",
      "27\n",
      "------------------------\n",
      "28\n",
      "------------------------\n",
      "29\n",
      "------------------------\n",
      "30\n",
      "------------------------\n",
      "31\n",
      "------------------------\n",
      "32\n",
      "------------------------\n",
      "33\n",
      "------------------------\n",
      "34\n",
      "------------------------\n",
      "35\n",
      "------------------------\n",
      "36\n",
      "------------------------\n",
      "37\n",
      "------------------------\n",
      "38\n",
      "------------------------\n",
      "39\n",
      "------------------------\n",
      "40\n",
      "------------------------\n",
      "41\n",
      "------------------------\n",
      "42\n",
      "------------------------\n",
      "43\n",
      "------------------------\n",
      "44\n",
      "------------------------\n",
      "45\n",
      "------------------------\n",
      "46\n",
      "------------------------\n",
      "47\n",
      "------------------------\n",
      "48\n",
      "------------------------\n",
      "49\n",
      "------------------------\n",
      "50\n",
      "------------------------\n",
      "51\n",
      "------------------------\n",
      "52\n",
      "------------------------\n",
      "53\n",
      "------------------------\n",
      "54\n",
      "------------------------\n",
      "55\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x8000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, Position, rows, cols = return_dynamic_fig(model_7250,cols=1, title=\"Distribution over input and output of rational activations\")\n",
    "palette = sns.color_palette('husl', 9)\n",
    "j=0\n",
    "for i,(n,m) in enumerate(model_7250.named_modules()):\n",
    "    if isinstance(m, Rational):\n",
    "        print(j)\n",
    "        ax1 = fig.add_subplot(rows,cols,Position[j])\n",
    "        ax2 = ax1.twinx()\n",
    "        m_np = np.array(deepcopy(m.input))\n",
    "        print(m_np.shape)\n",
    "        sns.histplot(m_np.flatten(), ax=ax1)\n",
    "        ax1.set_xlim(-3,3)\n",
    "        ax2.set_xlim(-3,3)\n",
    "        linsp = torch.linspace(-3,3,100).cuda()\n",
    "        \n",
    "        y = m.forward(linsp)\n",
    "        \n",
    "        sns.lineplot(x=linsp.detach().cpu().numpy(),y=y.detach().cpu().numpy(), ax=ax2, color=palette[1])\n",
    "        ax2.grid(False)\n",
    "        ax1.set_title(f'Input Distribution at Layer {j}')\n",
    "        j+=1\n",
    "        print('------------------------')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m y_labels \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mswitch_0\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mswitch_1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(res)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> 19\u001b[0m ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(rows,cols,Position[j])\n\u001b[1;32m     20\u001b[0m ax\u001b[39m.\u001b[39mhist(res[:,\u001b[39m0\u001b[39m],  bins\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mswitch_adapter\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m ax\u001b[39m.\u001b[39mset_title(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDistribution of Adapter Weights at Layer \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: range object index out of range"
     ]
    }
   ],
   "source": [
    "g = torch.distributions.Gumbel(0,1)\n",
    "switch_temp = 0.1\n",
    "\n",
    "# Create a matplot lib figure with subplots\n",
    "i = 0\n",
    "\n",
    "\n",
    "j = 0\n",
    "for n,p in model_7250.named_parameters():\n",
    "    if 'switch' in n: \n",
    "        res = []\n",
    "        for i in range(1000):\n",
    "            g_sample= g.sample(p.shape).to('cpu')\n",
    "            sim = torch.softmax((p + g_sample)/switch_temp, dim=-1)\n",
    "            res.append(sim)\n",
    "        \n",
    "        y_labels = ['switch_0', 'switch_1']\n",
    "        res = torch.stack(res).detach().cpu().numpy()\n",
    "        ax = fig.add_subplot(rows,cols,Position[j])\n",
    "        ax.hist(res[:,0],  bins=25, label='switch_adapter')\n",
    "        ax.set_title(f'Distribution of Adapter Weights at Layer {j//2}')\n",
    "        ax = fig.add_subplot(rows,cols,Position[(j+1)])\n",
    "        ax.set_title(f'Distribution of Identity Weights at Layer {j//2}')\n",
    "        ax.hist(res[:,1],  bins=25,label='switch_identity')\n",
    "        j += 2\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dd78b44acf79f242ecca224d05742a1b089d3d2b8416bfe26aa6b4466f63f70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
