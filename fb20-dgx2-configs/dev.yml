{
    name: Switch_Rational_LeakyRelu_Temp_0.5_GPT_NEO_1.3B,
    #  magma settings
    from_checkpoint: False, 
    # dtype: float32,
    perceiver: False,

    # image encoder settings
    encoder_name: 'clip_resnet_large',
    freeze_img_encoder: True,
    rational_image_encoder: False,
    few_shot: 3,

    # LM Settings
    lm_name: 'EleutherAI/gpt-neo-125M',
    fp16_enabled: False,

    # adapter settings
    # adapter_config: {"mlp": {"adapter_type": "normal", "downsample_factor": 4}, "switch_temp": 0.1, "adapter_switch": True, "hidden_act": "rational:relu" },
    cross_attention_config:{"cadence": 2, "n_latents":64},
    # train settings 
    batch_size: 1,
    train_steps: 500,
    lr: 1.0e-4,
    min_lr: 1.0e-9,
    lr_decay_iters: 300000,
    image_enc_lr: 1.0e-9,
    rationals_lr: 1.0e-7,
    switch_lr: 2.0e-9,
    use_image_embed_layernorm: true,
    image_embed_dropout_prob: 0.1, 
    image_size: 384,
    train_micro_batch_size_per_gpu: 1,
    
    gradient_accumulation_steps: 1,
    zero_stage: 2,
    gradient_clipping: 1.0,

    # dataset / save / load settings
    train_dataset_name: 'conceptual_captions',
    train_dataset_dir: '/home/ml-mmeuer/adaptable_magma/magma/datasets/coco_converted/coco_converted_index_dataset',
    eval_dataset_name: 'coco',
    eval_dataset_dir: '/home/ml-mmeuer/adaptable_magma/magma/datasets/coco_converted/coco_converted_index_dataset',
    
    save: "/home/ml-mmeuer/magma/model_checkpoints/multimodal_transformer_rn50x16",
    # load: "/mnt/shared_vol/checkpoints/multimodal_transformer_rn50x16",

    eval_every: 1000,
}