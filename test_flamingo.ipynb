{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magma.language_model import get_gptj\n",
    "from magma.config import MultimodalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magma.datasets import ImgCptDataset, collate_fn\n",
    "from magma.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_OUT_DIMS = {\n",
    "    \"nfresnet50\": 2048,\n",
    "    'clip_RN50': 2048,\n",
    "    \"clip\": 512,\n",
    "    \"clip_resnet\": 2560,\n",
    "    \"clip_resnet_large\": 3072,\n",
    "    \"clip_resnet_tiny\": 3072,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GatedCrossAttentionBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_attn_block \u001b[39m=\u001b[39m GatedCrossAttentionBlock(\n\u001b[1;32m      2\u001b[0m             config\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mcross_attention_config,\n\u001b[1;32m      3\u001b[0m             text_token_dim\u001b[39m=\u001b[39mlm\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m      4\u001b[0m             visual_token_dim\u001b[39m=\u001b[39mENCODER_OUT_DIMS[config\u001b[39m.\u001b[39mencoder_name],\n\u001b[1;32m      5\u001b[0m         )\u001b[39m.\u001b[39mto(DEVICE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GatedCrossAttentionBlock' is not defined"
     ]
    }
   ],
   "source": [
    "x_attn_block = GatedCrossAttentionBlock(\n",
    "            config=config.cross_attention_config,\n",
    "            text_token_dim=lm.config.hidden_size,\n",
    "            visual_token_dim=ENCODER_OUT_DIMS[config.encoder_name],\n",
    "        ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_attn_block.perceiver_pipe(latents, media_mask=media_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_attn_block' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_attn_block(word_embeddings)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_attn_block' is not defined"
     ]
    }
   ],
   "source": [
    "x_attn_block(word_embeddings).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 3, 3, 3]], device='cuda:2')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, tanh, einsum\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange, repeat\n",
    "from einops_exts import rearrange_many\n",
    "from magma.utils import get_world_info\n",
    "\n",
    "\n",
    "# class CrossAttentionTransformerBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         lm_block: nn.Module,\n",
    "#         config: dict,\n",
    "#         token_dim: int = 4096,\n",
    "#         **kwargs\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.lm_block = lm_block\n",
    "#         self.cross_x_block = GatedCrossAttentionBlock(\n",
    "#             config, token_dim, **kwargs)\n",
    "#         self.media_locations = None\n",
    "#         self.visual_features = None\n",
    "\n",
    "#     def forward(self, embs, **kwargs):\n",
    "#         logits = self.cross_x_block(\n",
    "#             embs, self.media_locations, self.visual_features)\n",
    "#         out = self.lm_block(logits, use_cache=False, **kwargs)\n",
    "#         return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: TensorType[\"Batch\", \"Sequence\", \"Dim\"]):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MaskedCrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_token_dim: int = 4096,\n",
    "        visual_token_dim: int = 2048,\n",
    "        num_heads: int = 8,\n",
    "        n_latents: int = 64, \n",
    "        head_dim: int = 64, \n",
    "        device: str = 'cuda:2' \n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        local_rank, rank, world_size = get_world_info()\n",
    "    #     self.device = device = f'cuda:{local_rank}' if local_rank is not None else 'cuda' if torch.cuda.is_available(\n",
    "    # ) else 'cpu'\n",
    "        self.device = DEVICE\n",
    "        self.num_heads = num_heads\n",
    "        self.temp = 1 / (head_dim ** -0.5)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "        self.v_k_w = nn.Linear(\n",
    "            visual_token_dim, head_dim * num_heads * 2, bias=False)\n",
    "        self.q_w = nn.Linear(\n",
    "            text_token_dim, head_dim * num_heads, bias=False)\n",
    "        self.out = nn.Linear(head_dim *num_heads, text_token_dim, bias=False)\n",
    "\n",
    "    def forward(self,\n",
    "                latent: TensorType[\"Batch\", \"Sequence\", \"TokenDim\"],\n",
    "                y: TensorType[\"Batch\", \"Sequence Length\", \"TokenDim\"],\n",
    "                media_mask: TensorType[\"Batch\", \"Sequence Length\"]\n",
    "                ):\n",
    "        visual_features = rearrange(latent, 'b t n d -> b (t n) d')\n",
    "        \n",
    "        k, v = self.v_k_w(visual_features).chunk(2, dim=-1)\n",
    "        q = self.q_w(y)\n",
    "        q, k, v = rearrange_many((q, k, v), 'b n (h d) -> b h n d', h=8)\n",
    "        sim = einsum('... i d, ... j d -> ... i j', q, k)\n",
    "        print('SIM SHAPE',sim.shape)\n",
    "        media_time = torch.arange(3).to(self.device) + 1\n",
    "        print('media_time',media_time.shape, media_time.device, media_mask.device)\n",
    "        text_to_media_mask = rearrange(media_mask, 'b i -> b 1 i 1') == repeat(media_time, 'j -> 1 1 1 (j m)', m=64)\n",
    "        print(text_to_media_mask.shape)\n",
    "        sim = sim.masked_fill(~text_to_media_mask, -torch.finfo(sim.dtype).max)\n",
    "        print(sim)\n",
    "        logits = sim.softmax(dim=-1)\n",
    "        print('logits',logits.shape)\n",
    "        text_without_media_mask = media_mask == 0\n",
    "        text_without_media_mask = rearrange(text_without_media_mask, 'b i -> b 1 i 1')\n",
    "        logits = logits.masked_fill(text_without_media_mask, 0.)\n",
    "        logits = einsum('... i j, ... j d -> ... i d', logits, v)\n",
    "        logits = rearrange(logits, 'b h n d -> b n (h d)')\n",
    "        y = self.out(logits)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedCrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, config, text_token_dim, visual_token_dim):\n",
    "        super().__init__()\n",
    "        self.x_attn = MaskedCrossAttention(\n",
    "            text_token_dim = text_token_dim,\n",
    "            visual_token_dim=visual_token_dim ,\n",
    "            n_latents = config['n_latents']\n",
    "            )\n",
    "        self.tanh1 = nn.Parameter(torch.tensor([0.]))\n",
    "        self.ffw = FeedForward(dim=text_token_dim)\n",
    "        self.tanh2 = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "    def perceiver_pipe(self, visual_features, media_mask):\n",
    "        self.media_mask = media_mask\n",
    "        self.visual_features = visual_features\n",
    "\n",
    "    def forward(self, embs: TensorType[\"Batch\", \"Sequence Length\", \"TokenDim\"]):\n",
    "        print('EMBS',embs.shape)\n",
    "\n",
    "        x_attn = self.x_attn(self.visual_features, embs, self.media_mask)\n",
    "        attn_out = embs + tanh(self.tanh1) * x_attn\n",
    "        x_ffw = attn_out + self.ffw(x_attn) * tanh(self.tanh2)\n",
    "\n",
    "        return x_ffw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CROSS ATTENTION\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "from einops_exts import rearrange_many\n",
    "\n",
    "\n",
    "class PerceiverAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_dim,\n",
    "        output_dim,\n",
    "        num_heads=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temp = 1 / (output_dim ** -0.5)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.v_k_w = torch.nn.Linear(\n",
    "            token_dim, output_dim * num_heads * 2, bias=False)\n",
    "        self.q_w = torch.nn.Linear(\n",
    "            token_dim, output_dim * num_heads, bias=False)\n",
    "        self.out = nn.Linear(output_dim*num_heads, token_dim, bias=False)\n",
    "\n",
    "    def forward(self, q: TensorType[\"Batch\", \"Number of Images\", \"OutputDim\", \"TokenDim\"], k_v: TensorType[\"Batch\", \"Number of Images\", \"Sequence\", \"TokenDim\"]):\n",
    "\n",
    "        # (batch, n_images, sequence_length + output_dim, embedding_dim)\n",
    "        k_v = torch.cat((k_v, q), dim=-2)\n",
    "        # (batch, n_images, sequence_length + output_dim, embedding_dim)\n",
    "        k, v = self.v_k_w(k_v).tensor_split(2, dim=-1)\n",
    "        q = self.q_w(q)  # (batch, n_images, output_dim, embedding_dim)\n",
    "\n",
    "        q, k, v = rearrange_many(\n",
    "            (q, k, v), 'b n s (h d) -> b n h s d', h=self.num_heads)\n",
    "\n",
    "        q = q * self.temp\n",
    "\n",
    "        # k = Image Input Sequence Dimension / q = Output Dimension\n",
    "        sm = torch.einsum(\"b n h q d,b n h k d->b n h q k\", q, k)\n",
    "        attn = sm.softmax(dim=-1)\n",
    "\n",
    "        ff_input_per_head = torch.einsum(\n",
    "            \"b n h q k,b n h k d -> b n h q d\", attn, v)\n",
    "        ff_input = rearrange(\n",
    "            ff_input_per_head, \"b n h s d -> b n s (h d)\", h=self.num_heads)\n",
    "        return self.out(ff_input)\n",
    "\n",
    "\n",
    "class PerceiverResampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_dim,\n",
    "        num_layers:  int = 2,\n",
    "        n_latents: int = 64,\n",
    "        time: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.register_parameter(\"learned_latents\", nn.Parameter(\n",
    "            torch.randn(n_latents, token_dim)))\n",
    "        self.register_parameter('time_embeddings', nn.Parameter(\n",
    "            torch.rand(time, 1, token_dim)))\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.perceiver_attention_layers = nn.ModuleList([])\n",
    "        for _ in range(num_layers):\n",
    "            self.perceiver_attention_layers.append(nn.ModuleList([\n",
    "                PerceiverAttentionBlock(\n",
    "                    token_dim=token_dim,  output_dim=n_latents),\n",
    "                PerceiverFeedForwardLayer(token_dim=token_dim)\n",
    "            ]))\n",
    "        self.normalize = nn.LayerNorm(token_dim)\n",
    "\n",
    "    def forward(self, x: TensorType[\"Batch\", \"Number of Images\", \"Time\", \"Sequence\", \"Token Dimesion\"]):\n",
    "        if x.ndim == 3:\n",
    "            x = x[:, None, None, :, :]\n",
    "\n",
    "        batch_size, time, number_of_images, sequence_length, token_dim = x.size()\n",
    "        x = rearrange(x, \"b t n s d -> b n (t s) d\")\n",
    "        latents = self.learned_latents.repeat(\n",
    "            batch_size, number_of_images, 1, 1)\n",
    "        x = x + self.time_embeddings[:number_of_images]\n",
    "\n",
    "        for att_module, ff_layer in self.perceiver_attention_layers:\n",
    "            latents = att_module(latents, x)\n",
    "            latents = ff_layer(latents)\n",
    "\n",
    "        return self.normalize(latents)\n",
    "\n",
    "\n",
    "class PerceiverFeedForwardLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_dim: int = 3027,\n",
    "        mult: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(token_dim)\n",
    "        self.inner = nn.Linear(token_dim, mult*token_dim, bias=False)\n",
    "        self.act = nn.GELU()\n",
    "        self.outer = nn.Linear(mult*token_dim, token_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.outer(self.act(self.inner(self.norm(x))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Callable, Union\n",
    "from torchtyping import patch_typeguard\n",
    "from einops import rearrange\n",
    "import timm\n",
    "from activations.torch import Rational\n",
    "import clip\n",
    "from clip.model import Bottleneck\n",
    "import copy\n",
    "\n",
    "from functools import partial\n",
    "from activations.utils.convert_network import convert_pytorch_model_to_rational\n",
    "import os\n",
    "\n",
    "# ----------------------------- Utils --------------------------------------\n",
    "\n",
    "clip.model.LayerNorm = (\n",
    "    nn.LayerNorm\n",
    ")  # we need to patch this for clip to work with deepspeed\n",
    "patch_typeguard()  # needed for torchtyping typechecks to work\n",
    "\n",
    "\n",
    "class Lambda(torch.nn.Module):\n",
    "    def __init__(self, fn: Callable):\n",
    "        super().__init__()\n",
    "        assert hasattr(fn, \"__call__\")\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x)\n",
    "\n",
    "\n",
    "# ------------------------- Image encoders ----------------------------------\n",
    "\n",
    "\n",
    "def nfresnet50(\n",
    "    device: Union[torch.device, str] = None, pretrained: bool = True, convert_to_rational: bool = False\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Loads nfresnet50 model, removing the pooling layer and replacing it with\n",
    "    an adaptive pooling layer.\n",
    "    \"\"\"\n",
    "    encoder = torch.nn.Sequential(\n",
    "        *list(timm.create_model(\"nf_resnet50\", pretrained=pretrained).children())[:-1]\n",
    "    )\n",
    "    pooling = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "    encoder = torch.nn.Sequential(encoder, pooling)\n",
    "    if device is not None:\n",
    "        encoder = encoder.to(device)\n",
    "    if convert_to_rational:\n",
    "        encoder = convert_pytorch_model_to_rational(encoder)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def clip_encoder(\n",
    "    device: Union[torch.device, str] = None, name: str = \"clip\", convert_to_rational: bool = False, approx_func: str = 'relu'\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Loads clip's image encoder module, discarding the lm component.\n",
    "\n",
    "    If the variant is a resnet model, we also remove the attention pooling.\n",
    "    \"\"\"\n",
    "    if name in [\"clip\", \"ViT-B/32\"]:\n",
    "        name = \"ViT-B/32\"\n",
    "    elif name in [\"clip_resnet\", \"RN50x4\"]:\n",
    "        name = \"RN50x4\"\n",
    "    elif name in ['clip_RN50']:\n",
    "        name = 'RN50'\n",
    "    elif name in [\"clip_resnet_large\",\"clip_resnet_tiny\", \"RN50x16\"]:\n",
    "        name = \"RN50x16\"\n",
    "    else:\n",
    "        raise ValueError(f\"encoder {name} not recognized\")\n",
    "\n",
    "    encoder = clip.load(name, device=device)[0].visual\n",
    "\n",
    "    # if device is not None:\n",
    "    #     encoder = encoder.to(device)\n",
    "\n",
    "    if \"RN\" in name:\n",
    "        # remove attention pooling\n",
    "        encoder.attnpool = Lambda(\n",
    "            partial(rearrange, pattern=\"b d h w -> b (h w) d\")\n",
    "        )  # remove attn pooling, just use reshaped features\n",
    "\n",
    "    if convert_to_rational:\n",
    "        encoder = convert_pytorch_model_to_rational(\n",
    "            encoder, rational_cuda=device, approx_func=\"relu\", submodule_class=Bottleneck)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def get_image_encoder(\n",
    "    name: str, device: Union[torch.device, str] = None, pretrained: bool = False, convert_to_rational: bool = False\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Loads image encoder module\n",
    "    \"\"\"\n",
    "    if name == \"nfresnet50\":\n",
    "        encoder = nfresnet50(device=device, pretrained=pretrained,\n",
    "                             convert_to_rational=convert_to_rational)\n",
    "    elif \"clip\" in name:\n",
    "        encoder = clip_encoder(device=device, name=name,\n",
    "                               convert_to_rational=convert_to_rational)\n",
    "    else:\n",
    "        raise ValueError(f\"image encoder {name} not recognized\")\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def add_cross_attention_modules(lm, config):\n",
    "    cross_attention_layers = []\n",
    "    for l in range(len(lm.transformer.h)):\n",
    "        layer_norm = getattr(lm.transformer.h[l], \"ln_1\")\n",
    "        x_attn_block = GatedCrossAttentionBlock(\n",
    "            config=config.cross_attention_config,\n",
    "            text_token_dim=lm.config.hidden_size,\n",
    "            visual_token_dim=ENCODER_OUT_DIMS[config.encoder_name],\n",
    "        )\n",
    "        cross_attention_layers.append(l)\n",
    "        setattr(lm.transformer.h[l], 'ln_1', nn.Sequential(\n",
    "            *[x_attn_block, layer_norm]))\n",
    "    return cross_attention_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = get_image_encoder('clip_resnet_large', device='cuda:2', convert_to_rational=False).cuda(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DEVICE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmagma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m get_transforms\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmagma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage_encoders\u001b[39;00m \u001b[39mimport\u001b[39;00m get_image_encoder\n\u001b[0;32m----> 4\u001b[0m enc \u001b[39m=\u001b[39m get_image_encoder(\u001b[39m'\u001b[39m\u001b[39mclip_resnet_large\u001b[39m\u001b[39m'\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m, convert_to_rational\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mcuda(DEVICE)\n\u001b[1;32m      5\u001b[0m transforms \u001b[39m=\u001b[39m get_transforms(\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclip_resnet_large\u001b[39m\u001b[39m'\u001b[39m, enc\u001b[39m.\u001b[39minput_resolution)\n\u001b[1;32m      6\u001b[0m config \u001b[39m=\u001b[39m MultimodalConfig\u001b[39m.\u001b[39mfrom_yml(\u001b[39m'\u001b[39m\u001b[39m/home/ml-mmeuer/adaptable_magma/fb20-dgx2-configs/dev.yml\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DEVICE' is not defined"
     ]
    }
   ],
   "source": [
    "from magma.transforms import get_transforms\n",
    "from magma.image_encoders import get_image_encoder\n",
    "\n",
    "enc = get_image_encoder('clip_resnet_large', device=DEVICE, convert_to_rational=False).cuda(DEVICE)\n",
    "transforms = get_transforms(0, 'clip_resnet_large', enc.input_resolution)\n",
    "config = MultimodalConfig.from_yml('/home/ml-mmeuer/adaptable_magma/fb20-dgx2-configs/dev.yml')\n",
    "lm = get_gptj(config, from_pretrained=config.lm_name).cuda(DEVICE)\n",
    "cross_attention_layers = add_cross_attention_modules(lm, config)\n",
    "perceiver_resampler = PerceiverResampler(\n",
    "                ENCODER_OUT_DIMS[config.encoder_name],\n",
    "                n_latents=config.cross_attention_config['n_latents'],\n",
    "            ).cuda(DEVICE)\n",
    "seq_len = lm.config.max_position_embeddings\n",
    "tokenizer = get_tokenizer(\n",
    "            config.tokenizer_name, sequence_length=seq_len)\n",
    "\n",
    "lm.resize_token_embeddings(len(tokenizer))\n",
    "word_embedding = lm.transformer.wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImgCptDataset(config.train_dataset_dir, tokenizer=tokenizer, transforms=transforms, few_shot=3)\n",
    "imgs, captions = data[0]\n",
    "imgs , captions = imgs.to('cuda:2'), captions.to('cuda:2')\n",
    "captions = captions[None, ...]\n",
    "enc_imgs = enc(imgs)[None,None, ...].to(DEVICE)\n",
    "latents = perceiver_resampler(enc_imgs)\n",
    "enc_captions = word_embedding(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = captions[captions != tokenizer.eos_token_id][None, ...]\n",
    "padding_needed = seq_len - captions.size(-1)\n",
    "eos_padding = torch.full((1, padding_needed), tokenizer.eos_token_id).to(DEVICE)\n",
    "captions_with_ids = torch.cat((captions, eos_padding), dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = word_embedding(captions_with_ids)\n",
    "media_pos = captions_with_ids == tokenizer.cls_token_id\n",
    "media_mask = media_pos.cumsum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in cross_attention_layers:\n",
    "    x_attn_block = getattr(\n",
    "        lm.transformer.h[l], 'ln_1')[0]\n",
    "\n",
    "    x_attn_block.perceiver_pipe(\n",
    "        latents, media_mask=media_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBS torch.Size([1, 2048, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cpu! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lm(inputs_embeds \u001b[39m=\u001b[39;49m word_embeddings, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:741\u001b[0m, in \u001b[0;36mGPTNeoForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    739\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 741\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    742\u001b[0m     input_ids,\n\u001b[1;32m    743\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    744\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    745\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    746\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    747\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    748\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    749\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    750\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    751\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    752\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    753\u001b[0m )\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    756\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:621\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    613\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    614\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    615\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m         head_mask[i],\n\u001b[1;32m    619\u001b[0m     )\n\u001b[1;32m    620\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m    623\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    624\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    625\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    626\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    627\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    628\u001b[0m     )\n\u001b[1;32m    630\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:326\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    318\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    324\u001b[0m ):\n\u001b[1;32m    325\u001b[0m     residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 326\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(hidden_states)\n\u001b[1;32m    327\u001b[0m     attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\n\u001b[1;32m    328\u001b[0m         hidden_states,\n\u001b[1;32m    329\u001b[0m         layer_past\u001b[39m=\u001b[39mlayer_past,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [21], line 111\u001b[0m, in \u001b[0;36mGatedCrossAttentionBlock.forward\u001b[0;34m(self, embs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, embs: TensorType[\u001b[39m\"\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSequence Length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTokenDim\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m    109\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEMBS\u001b[39m\u001b[39m'\u001b[39m,embs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 111\u001b[0m     x_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_attn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual_features, embs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmedia_mask)\n\u001b[1;32m    112\u001b[0m     attn_out \u001b[39m=\u001b[39m embs \u001b[39m+\u001b[39m tanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh1) \u001b[39m*\u001b[39m x_attn\n\u001b[1;32m    113\u001b[0m     x_ffw \u001b[39m=\u001b[39m attn_out \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffw(x_attn) \u001b[39m*\u001b[39m tanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh2)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [21], line 75\u001b[0m, in \u001b[0;36mMaskedCrossAttention.forward\u001b[0;34m(self, latent, y, media_mask)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     69\u001b[0m             latent: TensorType[\u001b[39m\"\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSequence\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTokenDim\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     70\u001b[0m             y: TensorType[\u001b[39m\"\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSequence Length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTokenDim\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     71\u001b[0m             media_mask: TensorType[\u001b[39m\"\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSequence Length\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     72\u001b[0m             ):\n\u001b[1;32m     73\u001b[0m     visual_features \u001b[39m=\u001b[39m rearrange(latent, \u001b[39m'\u001b[39m\u001b[39mb t n d -> b (t n) d\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m     k, v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_k_w(visual_features)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_w(y)\n\u001b[1;32m     77\u001b[0m     q, k, v \u001b[39m=\u001b[39m rearrange_many((q, k, v), \u001b[39m'\u001b[39m\u001b[39mb n (h d) -> b h n d\u001b[39m\u001b[39m'\u001b[39m, h\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cpu! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "lm(inputs_embeds = word_embeddings, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dd78b44acf79f242ecca224d05742a1b089d3d2b8416bfe26aa6b4466f63f70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
